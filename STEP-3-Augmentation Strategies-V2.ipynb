{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27aea986-feb9-4d04-a1c1-b48e8f12314f",
   "metadata": {},
   "source": [
    "### STEP 3: AUGMENTING .JSON FILES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bafbdb-9050-46db-a566-f7356c15856a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8138b1fb-6bc4-4ad7-afa1-ad280edf6e06",
   "metadata": {},
   "source": [
    "Safety Enhancements:\n",
    "=> Core function protection - Never rename crypto core functions\n",
    "\n",
    "Dependency-aware reordering - Better commutative operation safety\n",
    "\n",
    "\n",
    "=>  Sampling:\n",
    "Weighted strategy selection - Prioritize high-impact augmentations\n",
    "\n",
    "Balanced strategy distribution - Avoid overusing any single strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de06850d-2c05-4326-982d-4ff5aba7feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Unified PDV Processor\n",
    "# ----------------------------\n",
    "class UnifiedPDVProcessor:\n",
    "    def __init__(self):\n",
    "        self.feature_names = [\n",
    "            # Core parameters\n",
    "            'block_size', 'key_size', 'rounds',\n",
    "            # Cipher family indicators\n",
    "            'is_feistel', 'is_arx', 'is_spn', \n",
    "            # Operation counts (static - per round definition)\n",
    "            'xor_count', 'rotl_count', 'rotr_count', 'add_count', 'sub_count', \n",
    "            'and_count', 'sbox_count', 'perm_count',\n",
    "            # Structural complexity\n",
    "            'round_complexity', 'rotation_diversity', 'max_rotation_amount',\n",
    "            # Core function presence\n",
    "            'has_round_function', 'has_key_schedule', 'has_f_function', \n",
    "            'has_enc_round', 'has_dec_round',\n",
    "            # Graph statistics\n",
    "            'ast_node_count', 'ast_edge_count', 'function_count',\n",
    "            # Cryptographic properties\n",
    "            'uses_z_sequence', 'uses_shift_params', 'uses_sbox', 'uses_permutation',\n",
    "            # === ENRICHED FEATURES ===\n",
    "            'round_function_size', 'operations_per_round',\n",
    "            'complexity_ratio', 'estimated_total_operations',\n",
    "            'key_schedule_operations', 'encryption_operations', 'decryption_operations'\n",
    "        ]\n",
    "                # Define core round functions for each family\n",
    "        self.core_round_functions = {\n",
    "            'Feistel': ['F_function', 'simon_round'],\n",
    "            'ARX': ['speck_enc_round', 'speck_dec_round'],\n",
    "            'SPN': ['present_round', 'sbox_layer', 'p_layer']\n",
    "        }\n",
    "    \n",
    "    def create_unified_pdv(self, extracted_pdv: Dict, ast_data: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Convert family-specific PDV to unified schema using core round functions\"\"\"\n",
    "        \n",
    "        cipher_family = extracted_pdv.get(\"cipher_family\", \"\").lower()\n",
    "        is_feistel = 1 if \"feistel\" in cipher_family else 0\n",
    "        is_arx = 1 if \"arx\" in cipher_family else 0\n",
    "        is_spn = 1 if \"spn\" in cipher_family else 0\n",
    "        \n",
    "        # Get operation counts and structure\n",
    "        ops_summary = extracted_pdv.get(\"ops_summary\", {})\n",
    "        \n",
    "        # Extract features from the CORRECT structure based on cipher family\n",
    "        if is_feistel:\n",
    "            structure = extracted_pdv.get(\"feistel_structure\", {})\n",
    "        elif is_arx:\n",
    "            structure = extracted_pdv.get(\"arx_structure\", {})\n",
    "        elif is_spn:\n",
    "            structure = extracted_pdv.get(\"spn_structure\", {})\n",
    "        else:\n",
    "            structure = {}\n",
    "        \n",
    "        # Calculate enriched features using core round functions\n",
    "        rounds = extracted_pdv.get(\"rounds\", 0)\n",
    "        enriched_features = self._compute_enriched_features(ops_summary, structure, rounds, cipher_family)\n",
    "        \n",
    "        unified = {\n",
    "            # Core parameters\n",
    "            \"block_size\": extracted_pdv[\"block_size\"],\n",
    "            \"key_size\": extracted_pdv[\"key_size\"], \n",
    "            \"rounds\": rounds,\n",
    "            \n",
    "            # Family detection\n",
    "            \"is_feistel\": is_feistel,\n",
    "            \"is_arx\": is_arx,\n",
    "            \"is_spn\": is_spn,\n",
    "            \n",
    "            # Operation counts (static - per round definition)\n",
    "            \"xor_count\": ops_summary.get(\"xor_count\", 0),\n",
    "            \"rotl_count\": ops_summary.get(\"rotl_count\", 0),\n",
    "            \"rotr_count\": ops_summary.get(\"rotr_count\", 0),\n",
    "            \"add_count\": ops_summary.get(\"add_count\", 0),\n",
    "            \"sub_count\": ops_summary.get(\"sub_count\", 0),\n",
    "            \"and_count\": ops_summary.get(\"and_count\", 0),\n",
    "            \"sbox_count\": ops_summary.get(\"sbox_count\", 0),\n",
    "            \"perm_count\": ops_summary.get(\"perm_count\", 0),\n",
    "            \n",
    "            # Structural complexity\n",
    "            \"round_complexity\": self._extract_round_complexity(extracted_pdv, cipher_family),\n",
    "            \"rotation_diversity\": self._extract_rotation_diversity(extracted_pdv, cipher_family),\n",
    "            \"max_rotation_amount\": self._extract_max_rotation(extracted_pdv, cipher_family),\n",
    "            \n",
    "            # Core function presence\n",
    "            \"has_round_function\": self._extract_has_round_function(extracted_pdv, cipher_family),\n",
    "            \"has_f_function\": self._extract_has_f_function(extracted_pdv, cipher_family),\n",
    "            \"has_enc_round\": self._extract_has_enc_round(extracted_pdv, cipher_family),\n",
    "            \"has_dec_round\": self._extract_has_dec_round(extracted_pdv, cipher_family),\n",
    "            \"has_key_schedule\": self._extract_has_key_schedule(extracted_pdv, cipher_family),\n",
    "            \n",
    "            # Graph statistics\n",
    "            \"ast_node_count\": len(ast_data.get(\"nodes\", [])),\n",
    "            \"ast_edge_count\": len(ast_data.get(\"edges\", [])),\n",
    "            \"function_count\": len(ast_data.get(\"functions\", [])),\n",
    "            \n",
    "            # Cryptographic properties\n",
    "            \"uses_z_sequence\": ops_summary.get(\"z_seq_usage\", 0),\n",
    "            \"uses_shift_params\": self._extract_uses_shift_params(extracted_pdv, cipher_family),\n",
    "            \"uses_sbox\": 1 if ops_summary.get(\"sbox_count\", 0) > 0 else 0,\n",
    "            \"uses_permutation\": 1 if ops_summary.get(\"perm_count\", 0) > 0 else 0,\n",
    "            \n",
    "            # === ENRICHED FEATURES ===\n",
    "            **enriched_features\n",
    "        }\n",
    "        \n",
    "        return unified\n",
    "\n",
    "    def _compute_enriched_features(self, ops_summary: Dict, structure: Dict, rounds: int, cipher_family: str) -> Dict[str, Any]:\n",
    "        \"\"\"Compute enriched features using family-specific knowledge\"\"\"\n",
    "        \n",
    "        # Get round function size based on cipher family\n",
    "        round_function_size = self._get_round_function_size(ops_summary, cipher_family)\n",
    "        \n",
    "        # Operations that would execute per round\n",
    "        operations_per_round = round_function_size\n",
    "        \n",
    "        # Complexity ratio: operations per round relative to total rounds\n",
    "        complexity_ratio = operations_per_round / max(rounds, 1)\n",
    "        \n",
    "        # Estimated total operations (round function × rounds)\n",
    "        estimated_total_operations = operations_per_round * rounds if rounds > 0 else operations_per_round\n",
    "        \n",
    "        # Key schedule operations (family-specific estimates)\n",
    "        key_schedule_ops = self._get_key_schedule_operations(cipher_family, round_function_size)\n",
    "        \n",
    "        # Encryption vs decryption operations\n",
    "        enc_ops, dec_ops = self._get_enc_dec_operations(cipher_family, round_function_size)\n",
    "        \n",
    "        return {\n",
    "            \"round_function_size\": round_function_size,\n",
    "            \"operations_per_round\": operations_per_round,\n",
    "            \"complexity_ratio\": round(complexity_ratio, 4),\n",
    "            \"estimated_total_operations\": estimated_total_operations,\n",
    "            \"key_schedule_operations\": key_schedule_ops,\n",
    "            \"encryption_operations\": enc_ops,\n",
    "            \"decryption_operations\": dec_ops\n",
    "        }\n",
    "\n",
    "    def _get_round_function_size(self, ops_summary: Dict, cipher_family: str) -> int:\n",
    "        \"\"\"Get round function size based on cipher family and operation counts\"\"\"\n",
    "        # Family-specific round function size estimates\n",
    "        if \"feistel\" in cipher_family:\n",
    "            # Simon: F_function (3 ROTL + 1 AND + 1 XOR) + round logic ≈ 6-8\n",
    "            core_ops = ['xor_count', 'rotl_count', 'and_count']\n",
    "            return min(8, max(6, sum(ops_summary.get(op, 0) for op in core_ops)))\n",
    "            \n",
    "        elif \"arx\" in cipher_family:\n",
    "            # Speck: 1 ROTR + 1 ADD + 2 XOR + 1 ROTL ≈ 5-6\n",
    "            core_ops = ['xor_count', 'rotl_count', 'rotr_count', 'add_count']\n",
    "            return min(6, max(5, sum(ops_summary.get(op, 0) for op in core_ops)))\n",
    "            \n",
    "        elif \"spn\" in cipher_family:\n",
    "            # PRESENT: S-box + permutation + key mixing ≈ 2-3\n",
    "            core_ops = ['sbox_count', 'perm_count', 'xor_count']\n",
    "            return min(3, max(2, sum(ops_summary.get(op, 0) for op in core_ops)))\n",
    "            \n",
    "        else:\n",
    "            # Default conservative estimate\n",
    "            return sum(ops_summary.values()) // 2\n",
    "\n",
    "    def _get_key_schedule_operations(self, cipher_family: str, round_function_size: int) -> int:\n",
    "        \"\"\"Get key schedule operations based on cipher family\"\"\"\n",
    "        # Family-specific key schedule complexity estimates\n",
    "        if \"feistel\" in cipher_family:\n",
    "            return max(1, round_function_size // 2)  # Simon key schedule\n",
    "        elif \"arx\" in cipher_family:\n",
    "            return max(1, round_function_size // 2)  # Speck key schedule  \n",
    "        elif \"spn\" in cipher_family:\n",
    "            return 1  # PRESENT key schedule is simpler\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _get_enc_dec_operations(self, cipher_family: str, round_function_size: int) -> Tuple[int, int]:\n",
    "        \"\"\"Get encryption vs decryption operations based on cipher family\"\"\"\n",
    "        if \"feistel\" in cipher_family:\n",
    "            return round_function_size, round_function_size  # Simon: symmetric\n",
    "        elif \"arx\" in cipher_family:\n",
    "            return round_function_size, round_function_size  # Speck: symmetric\n",
    "        elif \"spn\" in cipher_family:\n",
    "            return round_function_size, round_function_size  # PRESENT: symmetric\n",
    "        else:\n",
    "            return round_function_size, round_function_size\n",
    "    #######\n",
    "\n",
    "    \n",
    "    def _extract_round_function_operations(self, ast_data: Dict) -> Dict[str, int]:\n",
    "        \"\"\"Actually parse and count operations in round functions only\"\"\"\n",
    "        nodes = ast_data.get(\"nodes\", [])\n",
    "        edges = ast_data.get(\"edges\", [])\n",
    "        \n",
    "        # Find round function nodes\n",
    "        round_function_nodes = self._find_round_functions(nodes)\n",
    "\n",
    "        print('round_function_nodes\\n\\n\\n', round_function_nodes)\n",
    "        \n",
    "        # Count operations within round functions only\n",
    "        round_function_ops = self._count_operations_in_round_functions(round_function_nodes, nodes, edges)\n",
    "        \n",
    "        return round_function_ops\n",
    "    \n",
    "    def _find_round_functions(self, nodes: List[Dict]) -> List[int]:\n",
    "        \"\"\"Find nodes that represent round functions\"\"\"\n",
    "        round_function_indicators = [\n",
    "            'simon_round', 'speck_enc_round', 'speck_dec_round', \n",
    "            'present_round', 'encrypt_iterate', 'F_function'\n",
    "        ]\n",
    "        \n",
    "        round_function_ids = []\n",
    "        for node in nodes:\n",
    "            label = str(node.get('label', '')).lower()\n",
    "            if any(indicator in label for indicator in round_function_indicators):\n",
    "                round_function_ids.append(node['id'])\n",
    "        \n",
    "        return round_function_ids\n",
    "    \n",
    "    def _count_operations_in_round_functions(self, round_function_ids: List[int], all_nodes: List[Dict], all_edges: List[Dict]) -> Dict[str, int]:\n",
    "        \"\"\"Count operations that are children of round functions\"\"\"\n",
    "        # Find all nodes that are contained within round functions\n",
    "        round_function_children = set()\n",
    "        \n",
    "        for edge in all_edges:\n",
    "            if edge.get('type') == 'contains' and edge['source'] in round_function_ids:\n",
    "                round_function_children.add(edge['target'])\n",
    "        \n",
    "        # Also include nodes reachable from round functions\n",
    "        visited = set(round_function_children)\n",
    "        queue = list(round_function_children)\n",
    "        \n",
    "        while queue:\n",
    "            current_id = queue.pop(0)\n",
    "            for edge in all_edges:\n",
    "                if edge['source'] == current_id and edge['target'] not in visited:\n",
    "                    visited.add(edge['target'])\n",
    "                    queue.append(edge['target'])\n",
    "        \n",
    "        # Count operations in these nodes\n",
    "        op_counts = {\n",
    "            'xor_count': 0, 'rotl_count': 0, 'rotr_count': 0,\n",
    "            'add_count': 0, 'sub_count': 0, 'and_count': 0,\n",
    "            'sbox_count': 0, 'perm_count': 0\n",
    "        }\n",
    "        \n",
    "        op_mapping = {\n",
    "            'XOR': 'xor_count',\n",
    "            'ROTL': 'rotl_count', \n",
    "            'ROTR': 'rotr_count',\n",
    "            'ADD': 'add_count',\n",
    "            'SUB': 'sub_count', \n",
    "            'AND': 'and_count',\n",
    "            'F_FUNCTION': 'sbox_count',  # For Simon\n",
    "            'LIST_REV': 'perm_count'     # For permutations\n",
    "        }\n",
    "        \n",
    "        for node_id in visited:\n",
    "            node = next((n for n in all_nodes if n['id'] == node_id), None)\n",
    "            if node and node.get('type') == 'op':\n",
    "                op_label = node.get('label', '')\n",
    "                if op_label in op_mapping:\n",
    "                    op_counts[op_mapping[op_label]] += 1\n",
    "                elif 'sbox' in op_label.lower():\n",
    "                    op_counts['sbox_count'] += 1\n",
    "                elif 'perm' in op_label.lower() or 'player' in op_label.lower():\n",
    "                    op_counts['perm_count'] += 1\n",
    "        \n",
    "        return op_counts\n",
    "    \n",
    "    def _estimate_key_schedule_operations(self, ast_data: Dict) -> int:\n",
    "        \"\"\"Estimate key schedule operation complexity from AST\"\"\"\n",
    "        nodes = ast_data.get(\"nodes\", [])\n",
    "        \n",
    "        # Look for key schedule functions\n",
    "        key_schedule_indicators = ['key_schedule', 'key_expansion', 'key_gen', 'gen_key_schedule']\n",
    "        has_key_schedule = any(\n",
    "            any(indicator in str(node.get('label', '')).lower() for indicator in key_schedule_indicators)\n",
    "            for node in nodes\n",
    "        )\n",
    "        \n",
    "        if has_key_schedule:\n",
    "            # Conservative estimate: key schedule is typically 1/3 to 1/2 of round function complexity\n",
    "            round_ops = self._extract_round_function_operations(ast_data)\n",
    "            round_complexity = sum(round_ops.values())\n",
    "            return max(1, round_complexity // 3)\n",
    "        return 0\n",
    "    \n",
    "    def _estimate_enc_dec_operations(self, round_function_ops: Dict, structure: Dict) -> Tuple[int, int]:\n",
    "        \"\"\"Estimate encryption vs decryption operation counts\"\"\"\n",
    "        round_complexity = sum(round_function_ops.values())\n",
    "        \n",
    "        # Simple heuristic based on cipher structure\n",
    "        has_dec_round = structure.get('has_dec_round', 0)\n",
    "        if has_dec_round:\n",
    "            # If separate decryption round exists, assume similar complexity\n",
    "            return round_complexity, round_complexity\n",
    "        else:\n",
    "            # If no separate decryption, assume decryption is inverse (similar complexity)\n",
    "            return round_complexity, round_complexity\n",
    "\n",
    "    def _extract_round_complexity(self, pdv: Dict, cipher_family: str) -> int:\n",
    "        \"\"\"Extract round complexity from the correct family-specific structure\"\"\"\n",
    "        if \"feistel\" in cipher_family:\n",
    "            return pdv.get(\"feistel_structure\", {}).get(\"f_function_complexity\", 0)\n",
    "        elif \"arx\" in cipher_family:\n",
    "            return pdv.get(\"arx_structure\", {}).get(\"enc_round_complexity\", 0)\n",
    "        elif \"spn\" in cipher_family:\n",
    "            return pdv.get(\"spn_structure\", {}).get(\"round_complexity\", 0)\n",
    "        return 0\n",
    "    \n",
    "    def _extract_rotation_diversity(self, pdv: Dict, cipher_family: str) -> int:\n",
    "        \"\"\"Extract rotation diversity from the correct family-specific structure\"\"\"\n",
    "        if \"feistel\" in cipher_family:\n",
    "            return pdv.get(\"feistel_structure\", {}).get(\"rotation_diversity\", 0)\n",
    "        elif \"arx\" in cipher_family:\n",
    "            return pdv.get(\"arx_structure\", {}).get(\"rotation_diversity\", 0)\n",
    "        return 0\n",
    " \n",
    "    def _extract_max_rotation(self, pdv: Dict, cipher_family: str) -> int:\n",
    "        \"\"\"Extract maximum rotation amount from the correct family-specific structure\"\"\"\n",
    "        if \"feistel\" in cipher_family:\n",
    "            return pdv.get(\"feistel_structure\", {}).get(\"max_rotation_amount\", 0)\n",
    "        elif \"arx\" in cipher_family:\n",
    "            return pdv.get(\"arx_structure\", {}).get(\"max_rotation_amount\", 0)\n",
    "        return 0\n",
    "    \n",
    "    def _extract_has_round_function(self, pdv: Dict, cipher_family: str) -> int:\n",
    "        \"\"\"Extract has_round_function from the correct family-specific structure\"\"\"\n",
    "        if \"feistel\" in cipher_family:\n",
    "            return pdv.get(\"feistel_structure\", {}).get(\"has_round_function\", 0)\n",
    "        elif \"arx\" in cipher_family:\n",
    "            return pdv.get(\"arx_structure\", {}).get(\"has_enc_round\", 0)\n",
    "        elif \"spn\" in cipher_family:\n",
    "            return pdv.get(\"spn_structure\", {}).get(\"has_round_function\", 0)\n",
    "        return 0\n",
    "    \n",
    "    def _extract_has_f_function(self, pdv: Dict, cipher_family: str) -> int:\n",
    "        \"\"\"Extract has_f_function - primarily for Feistel ciphers\"\"\"\n",
    "        if \"feistel\" in cipher_family:\n",
    "            return pdv.get(\"feistel_structure\", {}).get(\"has_f_function\", 0)\n",
    "        return 0\n",
    "    \n",
    "    def _extract_has_enc_round(self, pdv: Dict, cipher_family: str) -> int:\n",
    "        \"\"\"Extract has_enc_round - primarily for ARX ciphers\"\"\"\n",
    "        if \"arx\" in cipher_family:\n",
    "            return pdv.get(\"arx_structure\", {}).get(\"has_enc_round\", 0)\n",
    "        if \"feistel\" in cipher_family or \"spn\" in cipher_family:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    def _extract_has_dec_round(self, pdv: Dict, cipher_family: str) -> int:\n",
    "        \"\"\"Extract has_dec_round - primarily for ARX ciphers\"\"\"\n",
    "        if \"arx\" in cipher_family:\n",
    "            return pdv.get(\"arx_structure\", {}).get(\"has_dec_round\", 0)\n",
    "        if \"feistel\" in cipher_family or \"spn\" in cipher_family:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    def _extract_has_key_schedule(self, pdv: Dict, cipher_family: str) -> int:\n",
    "        \"\"\"Extract has_key_schedule from the correct family-specific structure\"\"\"\n",
    "        if \"feistel\" in cipher_family:\n",
    "            return pdv.get(\"feistel_structure\", {}).get(\"has_key_schedule\", 0)\n",
    "        elif \"arx\" in cipher_family:\n",
    "            return pdv.get(\"arx_structure\", {}).get(\"has_key_schedule\", 0)\n",
    "        elif \"spn\" in cipher_family:\n",
    "            return pdv.get(\"spn_structure\", {}).get(\"has_key_schedule\", 0)\n",
    "        return 0\n",
    "    \n",
    "    def _extract_uses_shift_params(self, pdv: Dict, cipher_family: str) -> int:\n",
    "        \"\"\"Extract uses_shift_params - primarily for ARX ciphers like Speck\"\"\"\n",
    "        if \"arx\" in cipher_family:\n",
    "            return 1 if pdv.get(\"shift_parameters\", {}).get(\"shift_params_defined\", False) else 0\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5532964b-d295-4a61-a2fb-d34930468b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Your existing utility functions are GOOD - keep them as-is\n",
    "def load_json(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON file with error handling.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data: Dict[str, Any], file_path: str):\n",
    "    \"\"\"Save JSON file with proper formatting.\"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def safe_truncate(text: str, max_length: int) -> str:\n",
    "    \"\"\"Safely truncate text to maximum length.\"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "    return text[:max_length-3] + \"...\"\n",
    "\n",
    "def recompute_op_counts_from_nodes_and_edges(ast: Dict[str, Any]):\n",
    "    \"\"\"Recompute operation counts from AST nodes after augmentation.\"\"\"\n",
    "    nodes = ast.get(\"nodes\", [])\n",
    "    \n",
    "    # Count operations from nodes\n",
    "    op_counts = {\n",
    "        \"xor_count\": 0, \"rotl_count\": 0, \"rotr_count\": 0,\n",
    "        \"add_count\": 0, \"sub_count\": 0, \"and_count\": 0,\n",
    "        \"sbox_count\": 0, \"perm_count\": 0\n",
    "    }\n",
    "    \n",
    "    for node in nodes:\n",
    "        if node.get(\"type\") == \"op\":\n",
    "            label = node.get(\"label\", \"\").upper()\n",
    "            if label == \"XOR\":\n",
    "                op_counts[\"xor_count\"] += 1\n",
    "            elif label == \"ROTL\":\n",
    "                op_counts[\"rotl_count\"] += 1\n",
    "            elif label == \"ROTR\":\n",
    "                op_counts[\"rotr_count\"] += 1\n",
    "            elif label == \"ADD\":\n",
    "                op_counts[\"add_count\"] += 1\n",
    "            elif label == \"SUB\":\n",
    "                op_counts[\"sub_count\"] += 1\n",
    "            elif label == \"AND\":\n",
    "                op_counts[\"and_count\"] += 1\n",
    "            elif \"SBOX\" in label:\n",
    "                op_counts[\"sbox_count\"] += 1\n",
    "            elif \"PERM\" in label:\n",
    "                op_counts[\"perm_count\"] += 1\n",
    "    \n",
    "    # Update the PDV\n",
    "    if \"pdv\" in ast and \"ops_summary\" in ast[\"pdv\"]:\n",
    "        ast[\"pdv\"][\"ops_summary\"].update(op_counts)\n",
    "\n",
    "def recompute_graph_stats(ast: Dict[str, Any]):\n",
    "    \"\"\"Recompute graph statistics after augmentation.\"\"\"\n",
    "    nodes = ast.get(\"nodes\", [])\n",
    "    edges = ast.get(\"edges\", [])\n",
    "    functions = ast.get(\"functions\", [])\n",
    "    \n",
    "    # Update unified PDV if it exists\n",
    "    if \"unified_pdv\" in ast:\n",
    "        ast[\"unified_pdv\"][\"ast_node_count\"] = len(nodes)\n",
    "        ast[\"unified_pdv\"][\"ast_edge_count\"] = len(edges)\n",
    "        ast[\"unified_pdv\"][\"function_count\"] = len(functions)\n",
    "\n",
    "# FIXED: Safer implementation that works with your actual AST structure\n",
    "def safe_representation_variation(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Change representation without changing semantics - SAFE VERSION\n",
    "    Only modifies metadata, not actual operations\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    # Add representation metadata to nodes without changing operations\n",
    "    for node in new_ast.get(\"nodes\", []):\n",
    "        if node.get(\"type\") in [\"op\", \"function\"]:\n",
    "            # Initialize features if not present\n",
    "            if \"features\" not in node:\n",
    "                node[\"features\"] = {}\n",
    "            \n",
    "            # Add representation variant as metadata only\n",
    "            variants = [\"direct\", \"decomposed\", \"optimized\", \"canonical\"]\n",
    "            node[\"features\"][\"representation_variant\"] = random.choice(variants)\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "# FIXED: Metadata-only versions for cipher-specific strategies\n",
    "def safe_present_sbox_representation(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Vary S-box representation in PRESENT - METADATA ONLY\n",
    "    \"\"\"\n",
    "    if ast.get(\"pdv\", {}).get(\"cipher_family\") != \"PRESENT\":\n",
    "        return ast\n",
    "    \n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    sbox_representations = [\"lookup_table\", \"computed_form\", \"bit_sliced\", \"algebraic_form\"]\n",
    "    \n",
    "    for node in new_ast.get(\"nodes\", []):\n",
    "        if (node.get(\"type\") == \"function\" and \n",
    "            any(sbox_keyword in node.get(\"label\", \"\").lower() \n",
    "                for sbox_keyword in [\"sbox\", \"present_sbox\"])):\n",
    "            \n",
    "            if \"features\" not in node:\n",
    "                node[\"features\"] = {}\n",
    "            node[\"features\"][\"sbox_representation\"] = random.choice(sbox_representations)\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "def safe_present_permutation_variation(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Vary permutation layer implementation - METADATA ONLY\n",
    "    \"\"\"\n",
    "    if ast.get(\"pdv\", {}).get(\"cipher_family\") != \"PRESENT\":\n",
    "        return ast\n",
    "    \n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    permutation_variants = [\"bitwise_mapping\", \"matrix_rotation\", \"word_operations\", \"parallel_blocks\"]\n",
    "    \n",
    "    for node in new_ast.get(\"nodes\", []):\n",
    "        if (node.get(\"type\") == \"function\" and \n",
    "            any(perm_keyword in node.get(\"label\", \"\").lower() \n",
    "                for perm_keyword in [\"p_layer\", \"permutation\"])):\n",
    "            \n",
    "            if \"features\" not in node:\n",
    "                node[\"features\"] = {}\n",
    "            node[\"features\"][\"permutation_implementation\"] = random.choice(permutation_variants)\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "def safe_present_round_structure_variation(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Vary PRESENT round function implementation - METADATA ONLY\n",
    "    \"\"\"\n",
    "    if ast.get(\"pdv\", {}).get(\"cipher_family\") != \"PRESENT\":\n",
    "        return ast\n",
    "    \n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    round_variants = [\"sequential\", \"integrated\", \"parallel\", \"pipelined\"]\n",
    "    \n",
    "    for node in new_ast.get(\"nodes\", []):\n",
    "        if (node.get(\"type\") == \"function\" and \n",
    "            \"present_round\" in node.get(\"label\", \"\").lower()):\n",
    "            \n",
    "            if \"features\" not in node:\n",
    "                node[\"features\"] = {}\n",
    "            node[\"features\"][\"round_implementation\"] = random.choice(round_variants)\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "def safe_key_schedule_representation(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Vary key schedule implementation - METADATA ONLY\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    variants = [\"recursive\", \"iterative\", \"unrolled\", \"table_based\"]\n",
    "    \n",
    "    for node in new_ast.get(\"nodes\", []):\n",
    "        if (node.get(\"type\") == \"function\" and \n",
    "            \"key\" in node.get(\"label\", \"\").lower() and \n",
    "            \"schedule\" in node.get(\"label\", \"\").lower()):\n",
    "            \n",
    "            if \"features\" not in node:\n",
    "                node[\"features\"] = {}\n",
    "            node[\"features\"][\"implementation_variant\"] = random.choice(variants)\n",
    "    \n",
    "    return new_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "954973d8-ab8c-4124-86bf-89e56c771d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced safe augmentations with improved safety and new strategies\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "from typing import Dict, Any, List, Set\n",
    "\n",
    "# -----------------------\n",
    "# IMPROVED CORE AUGMENTATIONS\n",
    "# -----------------------\n",
    "\n",
    "def safe_commutative_operation_reordering(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "     Reorder commutative operations with dependency checking\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    for func_name, op_sequence in new_ast.get(\"op_sequences\", {}).items():\n",
    "        if len(op_sequence) < 2:\n",
    "            continue\n",
    "            \n",
    "        new_sequence = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(op_sequence):\n",
    "            if op_sequence[i] in {\"XOR\", \"ADD\"}:\n",
    "                # Find consecutive commutative operations\n",
    "                j = i\n",
    "                while j < len(op_sequence) and op_sequence[j] in {\"XOR\", \"ADD\"}:\n",
    "                    j += 1\n",
    "                \n",
    "                # Only shuffle if no obvious dependencies in sequence\n",
    "                block = op_sequence[i:j]\n",
    "                if len(block) > 1 and _is_independent_block(block, func_name, new_ast):\n",
    "                    random.shuffle(block)\n",
    "                new_sequence.extend(block)\n",
    "                i = j\n",
    "            else:\n",
    "                new_sequence.append(op_sequence[i])\n",
    "                i += 1\n",
    "        \n",
    "        new_ast[\"op_sequences\"][func_name] = new_sequence\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "def _is_independent_block(block: List[str], func_name: str, ast: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if operations in block are likely independent\n",
    "    Conservative approach: assume dependent if complex structure\n",
    "    \"\"\"\n",
    "    # For now, use simple heuristic - allow shuffling for short blocks\n",
    "    return len(block) <= 4\n",
    "\n",
    "def safe_function_renaming(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "     Rename helper functions with crypto-core protection\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    # Core cryptographic functions to NEVER rename\n",
    "    CRYPTO_CORE_FUNCTIONS = {\n",
    "        \"encrypt\", \"decrypt\", \"round\", \"key\", \"schedule\", \"sbox\", \"p_layer\",\n",
    "        \"F_function\", \"simon_round\", \"speck_enc_round\", \"speck_dec_round\",\n",
    "        \"present_round\", \"present_sbox\", \"generate_key_schedule\"\n",
    "    }\n",
    "    \n",
    "    # Safe helper prefixes/suffixes\n",
    "    SAFE_HELPERS = {\"helper\", \"aux\", \"temp\", \"compute\", \"calculate\", \"process\"}\n",
    "    \n",
    "    rename_map = {}\n",
    "    nodes = new_ast.get(\"nodes\", [])\n",
    "    \n",
    "    # Identify safe functions to rename\n",
    "    for node in nodes:\n",
    "        if node.get(\"type\") == \"function\":\n",
    "            label = node.get(\"label\", \"\").lower()\n",
    "            \n",
    "            # Check if this is a crypto core function\n",
    "            is_core_function = any(core in label for core in CRYPTO_CORE_FUNCTIONS)\n",
    "            is_safe_helper = any(helper in label for helper in SAFE_HELPERS)\n",
    "            \n",
    "            if is_safe_helper and not is_core_function:\n",
    "                old_name = node[\"label\"]\n",
    "                new_name = f\"{old_name}_v{random.randint(1, 3)}\"\n",
    "                rename_map[old_name] = new_name\n",
    "                node[\"label\"] = new_name\n",
    "    \n",
    "    # Update references\n",
    "    if rename_map:\n",
    "        for node in nodes:\n",
    "            if node.get(\"type\") == \"function_call\" and node[\"label\"] in rename_map:\n",
    "                node[\"label\"] = rename_map[node[\"label\"]]\n",
    "        \n",
    "        # Update functions list\n",
    "        for func in new_ast.get(\"functions\", []):\n",
    "            if func[\"name\"] in rename_map:\n",
    "                func[\"name\"] = rename_map[func[\"name\"]]\n",
    "        \n",
    "        # Update op_sequences keys\n",
    "        if \"op_sequences\" in new_ast:\n",
    "            new_ops = {}\n",
    "            for k, v in new_ast[\"op_sequences\"].items():\n",
    "                new_ops[rename_map.get(k, k)] = v\n",
    "            new_ast[\"op_sequences\"] = new_ops\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "def safe_bit_operation_commutativity(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "     Safe bit operation reordering (removed directional shifts)\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    # ONLY truly commutative bit operations\n",
    "    commutative_bit_ops = {\n",
    "        \"bitwise_and\", \"bitwise_or\", \"bitwise_xor\"\n",
    "    }\n",
    "    \n",
    "    for func_name, op_sequence in new_ast.get(\"op_sequences\", {}).items():\n",
    "        if len(op_sequence) < 2:\n",
    "            continue\n",
    "            \n",
    "        new_sequence = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(op_sequence):\n",
    "            if op_sequence[i] in commutative_bit_ops:\n",
    "                # Find consecutive commutative operations\n",
    "                j = i\n",
    "                while j < len(op_sequence) and op_sequence[j] in commutative_bit_ops:\n",
    "                    j += 1\n",
    "                \n",
    "                # Shuffle this commutative block\n",
    "                block = op_sequence[i:j]\n",
    "                if len(block) > 1:\n",
    "                    random.shuffle(block)\n",
    "                new_sequence.extend(block)\n",
    "                i = j\n",
    "            else:\n",
    "                new_sequence.append(op_sequence[i])\n",
    "                i += 1\n",
    "        \n",
    "        new_ast[\"op_sequences\"][func_name] = new_sequence\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "# -----------------------\n",
    "# NEW AUGMENTATION STRATEGIES\n",
    "# -----------------------\n",
    "\n",
    "def safe_temporary_variable_renaming(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "     Rename temporary/local variables without changing logic\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    # Common temporary variable patterns\n",
    "    temp_patterns = {\"tmp\", \"temp\", \"x\", \"y\", \"z\", \"a\", \"b\", \"c\", \"var\", \"val\"}\n",
    "    \n",
    "    rename_map = {}\n",
    "    nodes = new_ast.get(\"nodes\", [])\n",
    "    \n",
    "    # Identify temporary variables to rename\n",
    "    for node in nodes:\n",
    "        if node.get(\"type\") == \"var\":\n",
    "            var_name = node.get(\"label\", \"\")\n",
    "            \n",
    "            # Only rename if it looks like a temporary\n",
    "            if (var_name.lower() in temp_patterns or \n",
    "                re.match(r'^[a-z]$', var_name) or  # Single letter\n",
    "                re.match(r'^tmp\\d*$', var_name.lower())):  # tmp123\n",
    "                \n",
    "                new_name = f\"{var_name}_{random.randint(1, 9)}\"\n",
    "                rename_map[var_name] = new_name\n",
    "                node[\"label\"] = new_name\n",
    "    \n",
    "    # Update variable references in function bodies\n",
    "    if rename_map:\n",
    "        for node in nodes:\n",
    "            if node.get(\"type\") == \"function\" and \"body_text\" in node.get(\"features\", {}):\n",
    "                body = node[\"features\"][\"body_text\"]\n",
    "                for old_name, new_name in rename_map.items():\n",
    "                    # Use word boundaries to avoid partial replacements\n",
    "                    body = re.sub(r'\\b' + re.escape(old_name) + r'\\b', new_name, body)\n",
    "                node[\"features\"][\"body_text\"] = safe_truncate(body, 300)\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "def safe_comment_whitespace_injection(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "     Add synthetic comments and whitespace variations\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    comment_templates = [\n",
    "        \"(* Helper computation *)\",\n",
    "        \"(* Temporary variable *)\", \n",
    "        \"(* Cryptographic operation *)\",\n",
    "        \"(* Bit manipulation *)\",\n",
    "        \"(* Round function component *)\"\n",
    "    ]\n",
    "    \n",
    "    for node in new_ast.get(\"nodes\", []):\n",
    "        if node.get(\"type\") == \"function\" and \"body_text\" in node.get(\"features\", {}):\n",
    "            body = node[\"features\"][\"body_text\"]\n",
    "            \n",
    "            # Occasionally add a comment at the beginning\n",
    "            if random.random() < 0.3:\n",
    "                comment = random.choice(comment_templates)\n",
    "                body = f\"{comment}\\n{body}\"\n",
    "            \n",
    "            # Add some whitespace variations\n",
    "            if random.random() < 0.4:\n",
    "                # Randomly add/remove some newlines\n",
    "                lines = body.split('\\n')\n",
    "                if len(lines) > 2:\n",
    "                    # Occasionally add an extra newline\n",
    "                    if random.random() < 0.3:\n",
    "                        insert_pos = random.randint(1, len(lines)-1)\n",
    "                        lines.insert(insert_pos, \"\")\n",
    "                    body = '\\n'.join(lines)\n",
    "            \n",
    "            node[\"features\"][\"body_text\"] = safe_truncate(body, 300)\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "def safe_annotation_augmentation(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "     Add synthetic Isabelle annotations\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    annotations = [\n",
    "        \"(*@ verified *)\",\n",
    "        \"(*@ inline *)\", \n",
    "        \"(*@ pure *)\",\n",
    "        \"(*@ preserves_crypto *)\",\n",
    "        \"(*@ preserves_semantics *)\"\n",
    "    ]\n",
    "    \n",
    "    for node in new_ast.get(\"nodes\", []):\n",
    "        if node.get(\"type\") == \"function\" and \"body_text\" in node.get(\"features\", {}):\n",
    "            body = node[\"features\"][\"body_text\"]\n",
    "            \n",
    "            # Add annotation with some probability\n",
    "            if random.random() < 0.25:\n",
    "                annotation = random.choice(annotations)\n",
    "                body = f\"{annotation}\\n{body}\"\n",
    "                node[\"features\"][\"body_text\"] = safe_truncate(body, 300)\n",
    "            \n",
    "            # Add annotation metadata to features\n",
    "            features = node.get(\"features\", {})\n",
    "            if \"annotations\" not in features:\n",
    "                features[\"annotations\"] = []\n",
    "            if random.random() < 0.2:\n",
    "                features[\"annotations\"].append(random.choice([\"verified\", \"pure\", \"inline\"]))\n",
    "            node[\"features\"] = features\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "def safe_function_inlining(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "     Replace simple helper function calls with metadata\n",
    "    (Doesn't actually inline, just marks for representation)\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    for node in new_ast.get(\"nodes\", []):\n",
    "        if node.get(\"type\") == \"function_call\":\n",
    "            # Mark some calls as \"inlinable\" in metadata\n",
    "            func_name = node.get(\"label\", \"\")\n",
    "            \n",
    "            # Simple heuristics for inlinable functions\n",
    "            is_simple_helper = any(pattern in func_name.lower() \n",
    "                                 for pattern in [\"helper\", \"compute\", \"calculate\", \"get\"])\n",
    "            \n",
    "            if is_simple_helper and random.random() < 0.3:\n",
    "                features = node.get(\"features\", {})\n",
    "                features[\"inlinable\"] = True\n",
    "                features[\"inline_variant\"] = random.choice([\"direct\", \"expanded\", \"optimized\"])\n",
    "                node[\"features\"] = features\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "# -----------------------\n",
    "# ENHANCED STRATEGY LISTS\n",
    "# -----------------------\n",
    "\n",
    "# Core strategies (high impact, very safe)\n",
    "CORE_AUG_STRATEGIES = [\n",
    "    safe_commutative_operation_reordering,\n",
    "    safe_function_renaming,\n",
    "    safe_temporary_variable_renaming,\n",
    "    safe_representation_variation,\n",
    "]\n",
    "\n",
    "# Textual variation strategies (safe, adds diversity)\n",
    "TEXTUAL_AUG_STRATEGIES = [\n",
    "    safe_comment_whitespace_injection,\n",
    "    safe_annotation_augmentation,\n",
    "]\n",
    "\n",
    "# Cipher-specific strategies\n",
    "PRESENT_SPECIFIC_STRATEGIES = CORE_AUG_STRATEGIES + [\n",
    "    safe_bit_operation_commutativity,\n",
    "    safe_present_sbox_representation,\n",
    "    safe_present_permutation_variation,\n",
    "    safe_present_round_structure_variation,\n",
    "] + TEXTUAL_AUG_STRATEGIES\n",
    "\n",
    "SPECK_SPECIFIC_STRATEGIES = CORE_AUG_STRATEGIES + [\n",
    "    safe_key_schedule_representation,\n",
    "] + TEXTUAL_AUG_STRATEGIES\n",
    "\n",
    "SIMON_SPECIFIC_STRATEGIES = CORE_AUG_STRATEGIES + [\n",
    "    safe_key_schedule_representation, \n",
    "] + TEXTUAL_AUG_STRATEGIES\n",
    "\n",
    "AUGMENTATION_STRATEGIES = {\n",
    "    \"Simon\": SIMON_SPECIFIC_STRATEGIES,\n",
    "    \"Speck\": SPECK_SPECIFIC_STRATEGIES,\n",
    "    \"PRESENT\": PRESENT_SPECIFIC_STRATEGIES\n",
    "}\n",
    "\n",
    "# Strategy weights for sampling (higher = more frequent)\n",
    "STRATEGY_WEIGHTS = {\n",
    "    safe_commutative_operation_reordering: 2.0,  # High impact\n",
    "    safe_function_renaming: 1.5,                 # Medium impact  \n",
    "    safe_temporary_variable_renaming: 1.2,       # Medium impact\n",
    "    safe_representation_variation: 1.0,          # Medium impact\n",
    "    safe_comment_whitespace_injection: 0.8,      # Lower impact\n",
    "    safe_annotation_augmentation: 0.7,           # Lower impact\n",
    "    safe_function_inlining: 0.6,                 # Lower impact\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def weighted_strategy_sampling(strategies: List, num_strategies: int) -> List:\n",
    "    \"\"\"\n",
    "    Sample strategies based on weights for more intelligent augmentation\n",
    "    \"\"\"\n",
    "    if not strategies:\n",
    "        return []\n",
    "    \n",
    "    # Get weights for available strategies\n",
    "    available_weights = [STRATEGY_WEIGHTS.get(s, 1.0) for s in strategies]\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = sum(available_weights)\n",
    "    if total_weight == 0:\n",
    "        return random.sample(strategies, min(num_strategies, len(strategies)))\n",
    "    \n",
    "    normalized_weights = [w / total_weight for w in available_weights]\n",
    "    \n",
    "    # Sample without replacement using weights\n",
    "    selected = []\n",
    "    remaining_strategies = strategies.copy()\n",
    "    remaining_weights = normalized_weights.copy()\n",
    "    \n",
    "    for _ in range(min(num_strategies, len(strategies))):\n",
    "        if not remaining_strategies:\n",
    "            break\n",
    "            \n",
    "        # Weighted random choice\n",
    "        chosen_idx = random.choices(\n",
    "            range(len(remaining_strategies)), \n",
    "            weights=remaining_weights\n",
    "        )[0]\n",
    "        \n",
    "        selected.append(remaining_strategies[chosen_idx])\n",
    "        \n",
    "        # Remove selected strategy\n",
    "        remaining_strategies.pop(chosen_idx)\n",
    "        remaining_weights.pop(chosen_idx)\n",
    "        \n",
    "        # Re-normalize weights\n",
    "        total_remaining = sum(remaining_weights)\n",
    "        if total_remaining > 0:\n",
    "            remaining_weights = [w / total_remaining for w in remaining_weights]\n",
    "    \n",
    "    return selected\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e54192-c508-4391-9f35-793a49b25387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e00609-1ff5-4e60-8c86-f66da203c9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "948e38ac-3dd6-443e-8389-38fabb860bdd",
   "metadata": {},
   "source": [
    "## additional structural and metadata augmentation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c833768-8702-47c5-96e8-e94cf45dded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Add these to your augmentation script)\n",
    "import re\n",
    "\n",
    "def safe_feature_noise_injection(ast: Dict[str, Any], noise_level: float = 0.05) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Add small Gaussian noise to continuous numerical features.\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    \n",
    "    for node in new_ast.get(\"nodes\", []):\n",
    "        # Add noise to the root numerical features\n",
    "        for key in [\"crypto_strength\", \"diffusion_power\", \"nonlinearity\"]:\n",
    "            if key in node and isinstance(node[key], (int, float)):\n",
    "                if node[key] > 0: # Only add noise to non-zero features\n",
    "                    noise = random.gauss(0, noise_level)\n",
    "                    node[key] = max(0, node[key] * (1 + noise)) # Ensure non-negative\n",
    "\n",
    "        # Add noise to features in the 'features' sub-dict (if you use it)\n",
    "        if \"features\" in node and isinstance(node[\"features\"], dict):\n",
    "            for key, value in node[\"features\"].items():\n",
    "                if isinstance(value, (int, float)) and value > 0 and random.random() < 0.2:\n",
    "                    noise = random.gauss(0, noise_level)\n",
    "                    node[\"features\"][key] = max(0, value * (1 + noise))\n",
    "    \n",
    "    return new_ast\n",
    "\n",
    "def safe_node_dropout_cryptographic(ast: Dict[str, Any], p: float = 0.15) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enhanced node dropout that understands cryptographic semantics\n",
    "    (Deepseek's excellent version)\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    nodes = new_ast.get(\"nodes\", [])\n",
    "    \n",
    "    if len(nodes) < 15:  # Don't augment very small graphs\n",
    "        return new_ast\n",
    "\n",
    "    # CRYPTOGRAPHIC SEMANTICS: Define what can NEVER be dropped\n",
    "    NEVER_DROP_ROLES = {\n",
    "        \"feistel_f_function\", \"sbox_substitution\", \"permutation_layer\",\n",
    "        \"modular_addition\", \"nonlinear_mixing\"\n",
    "    }\n",
    "    \n",
    "    NEVER_DROP_LABELS = {\n",
    "        \"round\", \"encrypt\", \"decrypt\", \"key_schedule\", \"F_function\",\n",
    "        \"sbox\", \"p_layer\", \"present_round\", \"simon_round\", \"speck_enc_round\"\n",
    "    }\n",
    "\n",
    "    # Classify nodes by cryptographic importance\n",
    "    critical_node_ids = set()\n",
    "    droppable_nodes = []\n",
    "\n",
    "    for node in nodes:\n",
    "        is_critical = False\n",
    "        if node.get(\"crypto_role\") in NEVER_DROP_ROLES:\n",
    "            is_critical = True\n",
    "        for label in NEVER_DROP_LABELS:\n",
    "            if label in node.get(\"label\", \"\").lower():\n",
    "                is_critical = True\n",
    "        \n",
    "        if is_critical:\n",
    "            critical_node_ids.add(node[\"id\"])\n",
    "        else:\n",
    "            droppable_nodes.append(node)\n",
    "\n",
    "    # Apply dropout with cryptographic awareness\n",
    "    if droppable_nodes:\n",
    "        num_to_drop = max(1, int(len(droppable_nodes) * p))\n",
    "        nodes_to_drop = random.sample(droppable_nodes, num_to_drop)\n",
    "        ids_to_drop = {node[\"id\"] for node in nodes_to_drop}\n",
    "        \n",
    "        new_ast[\"nodes\"] = [n for n in nodes if n[\"id\"] not in ids_to_drop]\n",
    "        new_ast[\"edges\"] = [\n",
    "            e for e in new_ast.get(\"edges\", [])\n",
    "            if e[\"source\"] not in ids_to_drop and e[\"target\"] not in ids_to_drop\n",
    "        ]\n",
    "        \n",
    "        # Track what was dropped for analysis\n",
    "        if \"pdv\" not in new_ast: new_ast[\"pdv\"] = {}\n",
    "        if \"augmentation_metadata\" not in new_ast[\"pdv\"]: new_ast[\"pdv\"][\"augmentation_metadata\"] = {}\n",
    "        \n",
    "        aug_meta = new_ast[\"pdv\"][\"augmentation_metadata\"]\n",
    "        aug_meta[\"node_dropout_count\"] = num_to_drop\n",
    "        aug_meta[\"dropped_node_types\"] = list(set(n.get(\"type\") for n in nodes_to_drop))\n",
    "\n",
    "    return new_ast\n",
    "\n",
    "def safe_edge_perturbation_cryptographic(ast: Dict[str, Any], p_drop: float = 0.08, p_add: float = 0.05) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Edge perturbation that respects cryptographic data flow\n",
    "    (Deepseek's excellent version)\n",
    "    \"\"\"\n",
    "    new_ast = copy.deepcopy(ast)\n",
    "    nodes = new_ast.get(\"nodes\", [])\n",
    "    edges = new_ast.get(\"edges\", [])\n",
    "    \n",
    "    if not edges:\n",
    "        return new_ast\n",
    "\n",
    "    # NEVER drop these critical edge types\n",
    "    CRITICAL_EDGE_TYPES = {\"contains\", \"func\", \"amount\"}  # Structural edges\n",
    "    \n",
    "    # Safe to drop these edge types\n",
    "    DROPPABLE_EDGE_TYPES = {\"arg\", \"child\", \"left\", \"right\", \"binding\", \"body\"}\n",
    "    \n",
    "    # Separate edges by importance\n",
    "    critical_edges = [e for e in edges if e.get(\"type\") in CRITICAL_EDGE_TYPES]\n",
    "    droppable_edges = [e for e in edges if e.get(\"type\") in DROPPABLE_EDGE_TYPES]\n",
    "    \n",
    "    # Edge dropping (only from droppable set)\n",
    "    num_edges_to_drop = max(1, int(len(droppable_edges) * p_drop))\n",
    "    remaining_edges = critical_edges\n",
    "    if droppable_edges and num_edges_to_drop > 0:\n",
    "        edges_to_keep = random.sample(droppable_edges, len(droppable_edges) - num_edges_to_drop)\n",
    "        remaining_edges.extend(edges_to_keep)\n",
    "    else:\n",
    "        remaining_edges.extend(droppable_edges)\n",
    "    \n",
    "    # Edge addition (create semantically plausible new connections)\n",
    "    new_edges = []\n",
    "    num_edges_to_add = max(1, int(len(edges) * p_add))\n",
    "    \n",
    "    source_candidates = [n[\"id\"] for n in nodes if n.get(\"type\") in [\"op\", \"function\", \"var\", \"literal\"]]\n",
    "    target_candidates = [n[\"id\"] for n in nodes if n.get(\"type\") in [\"op\", \"function\"]]\n",
    "    \n",
    "    for _ in range(num_edges_to_add):\n",
    "        if source_candidates and target_candidates:\n",
    "            source_id = random.choice(source_candidates)\n",
    "            target_id = random.choice(target_candidates)\n",
    "            \n",
    "            if (source_id != target_id and\n",
    "                not any(e[\"source\"] == source_id and e[\"target\"] == target_id \n",
    "                        for e in remaining_edges + new_edges)):\n",
    "                \n",
    "                new_edge = {\n",
    "                    \"source\": source_id,\n",
    "                    \"target\": target_id,\n",
    "                    \"type\": \"arg\",  # A plausible, droppable type\n",
    "                    \"features\": {\"synthetic\": True, \"augmentation\": \"edge_add\"}\n",
    "                }\n",
    "                new_edges.append(new_edge)\n",
    "    \n",
    "    new_ast[\"edges\"] = remaining_edges + new_edges\n",
    "    \n",
    "    # Update metadata\n",
    "    if \"pdv\" not in new_ast: new_ast[\"pdv\"] = {}\n",
    "    if \"augmentation_metadata\" not in new_ast[\"pdv\"]: new_ast[\"pdv\"][\"augmentation_metadata\"] = {}\n",
    "    \n",
    "    aug_meta = new_ast[\"pdv\"][\"augmentation_metadata\"]\n",
    "    aug_meta[\"edge_perturb_drop\"] = num_edges_to_drop\n",
    "    aug_meta[\"edge_perturb_add\"] = len(new_edges)\n",
    "\n",
    "    return new_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d6e86a4-b9a6-4973-868c-cc36721dd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your augmentation script:\n",
    "\n",
    "# --- NEW STRATEGY LISTS ---\n",
    "# Core strategies (high impact, very safe)\n",
    "CORE_AUG_STRATEGIES = [\n",
    "    safe_node_dropout_cryptographic,          #\n",
    "    safe_edge_perturbation_cryptographic,     # \n",
    "    safe_commutative_operation_reordering,\n",
    "    safe_function_renaming,\n",
    "    safe_temporary_variable_renaming,\n",
    "    safe_representation_variation,\n",
    "]\n",
    "\n",
    "# Textual/Noise variation strategies (safe, adds diversity)\n",
    "TEXTUAL_AUG_STRATEGIES = [\n",
    "    safe_feature_noise_injection,             # \n",
    "    safe_comment_whitespace_injection,\n",
    "    safe_annotation_augmentation,\n",
    "]\n",
    "\n",
    "# Cipher-specific strategies\n",
    "PRESENT_SPECIFIC_STRATEGIES = CORE_AUG_STRATEGIES + [\n",
    "    safe_bit_operation_commutativity,\n",
    "    safe_present_sbox_representation,\n",
    "    safe_present_permutation_variation,\n",
    "    safe_present_round_structure_variation,\n",
    "] + TEXTUAL_AUG_STRATEGIES\n",
    "\n",
    "SPECK_SPECIFIC_STRATEGIES = CORE_AUG_STRATEGIES + [\n",
    "    safe_key_schedule_representation,\n",
    "] + TEXTUAL_AUG_STRATEGIES\n",
    "\n",
    "SIMON_SPECIFIC_STRATEGIES = CORE_AUG_STRATEGIES + [\n",
    "    safe_key_schedule_representation,  \n",
    "] + TEXTUAL_AUG_STRATEGIES\n",
    "\n",
    "AUGMENTATION_STRATEGIES = {\n",
    "    \"Simon\": SIMON_SPECIFIC_STRATEGIES,\n",
    "    \"Speck\": SPECK_SPECIFIC_STRATEGIES,\n",
    "    \"PRESENT\": PRESENT_SPECIFIC_STRATEGIES\n",
    "}\n",
    "\n",
    "# --- NEW WEIGHTS ---\n",
    "STRATEGY_WEIGHTS = {\n",
    "    # Structural (Highest Impact)\n",
    "    safe_node_dropout_cryptographic: 3.0,\n",
    "    safe_edge_perturbation_cryptographic: 2.5,\n",
    "    safe_commutative_operation_reordering: 2.0,\n",
    "    \n",
    "    # Metadata (Medium Impact)\n",
    "    safe_function_renaming: 1.5,\n",
    "    safe_temporary_variable_renaming: 1.2,\n",
    "    safe_representation_variation: 1.0,\n",
    "\n",
    "    # Noise/Textual (Regularization)\n",
    "    safe_feature_noise_injection: 1.0,\n",
    "    safe_comment_whitespace_injection: 0.5,\n",
    "    safe_annotation_augmentation: 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f8491-6224-4282-be3e-9bb591b91771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your augmentation script:\n",
    "# (Make sure to import all your augmentation functions)\n",
    "\n",
    "\n",
    "# Configuration\n",
    "AUGMENTED_DIR = \"augmented_data_V6\"\n",
    "AUG_PER_FILE = 10 # Number of augmented variants per original file\n",
    "\n",
    "SOURCE_DIR_ = 'output_ast_V5'\n",
    "SIMON_INPUT = \"output_ast_V5/Simon\"\n",
    "SPECK_INPUT = \"output_ast_V5/Speck\"\n",
    "PRESENT_INPUT = \"output_ast_V5/PRESENT\"\n",
    "\n",
    "\n",
    "def run_progressive_augmentation():\n",
    "    \"\"\"\n",
    "    Creates multiple augmented datasets with increasing size\n",
    "    based on the 23 original files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your 23 original files are in \"dataset-1\"\n",
    "    SOURCE_DIR = SOURCE_DIR_ \n",
    "    \n",
    "    # We will create 3 new output directories\n",
    "    OUTPUT_BASE_DIR = AUGMENTED_DIR\n",
    "\n",
    "    CIPHERS = [\"Simon\", \"Speck\", \"PRESENT\"]\n",
    "    \n",
    "    # This is Deepseek's \"Progressive Scaling\" plan\n",
    "    PROGRESSIVE_AUGMENTATION_PLANS = {\n",
    "        'dataset-8x': {\n",
    "            'augmentations_per_file': 7,  # 7 augs + 1 original = 8x\n",
    "            'node_dropout_p': 0.1,\n",
    "            'edge_perturb_p': 0.05,\n",
    "            'noise_level': 0.02\n",
    "        },\n",
    "        'dataset-15x': {\n",
    "            'augmentations_per_file': 14, # 14 augs + 1 original = 15x\n",
    "            'node_dropout_p': 0.15,\n",
    "            'edge_perturb_p': 0.08,\n",
    "            'noise_level': 0.05\n",
    "        },\n",
    "        'dataset-30x': {\n",
    "            'augmentations_per_file': 29, # 29 augs + 1 original = 30x\n",
    "            'node_dropout_p': 0.2,\n",
    "            'edge_perturb_p': 0.1,\n",
    "            'noise_level': 0.05\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for plan_name, plan_config in PROGRESSIVE_AUGMENTATION_PLANS.items():\n",
    "        print(f\"\\n--- Generating {plan_name} ---\")\n",
    "        num_augs = plan_config['augmentations_per_file']\n",
    "        #output_dir_plan = os.path.join(OUTPUT_BASE_DIR, plan_name)\n",
    "        output_dir_plan = os.path.join(OUTPUT_BASE_DIR, '')\n",
    "        \n",
    "        total_graphs = 0\n",
    "        \n",
    "        for cipher_name in CIPHERS:\n",
    "            source_cipher_dir = os.path.join(SOURCE_DIR, cipher_name)\n",
    "            output_cipher_dir = os.path.join(output_dir_plan, cipher_name)\n",
    "            \n",
    "            if not os.path.exists(source_cipher_dir):\n",
    "                print(f\"Warning: Source dir not found: {source_cipher_dir}\")\n",
    "                continue\n",
    "\n",
    "            strategies = AUGMENTATION_STRATEGIES.get(cipher_name, CORE_AUG_STRATEGIES)\n",
    "            json_files = [f for f in os.listdir(source_cipher_dir) \n",
    "                          if f.endswith(\".json\") and not f.startswith(\"_\")]\n",
    "            \n",
    "            print(f\"Processing {cipher_name}: {len(json_files)} original files...\")\n",
    "\n",
    "            for json_file in json_files:\n",
    "                input_path = os.path.join(source_cipher_dir, json_file)\n",
    "                try:\n",
    "                    original_ast = load_json(input_path)\n",
    "                    \n",
    "                    # Create N augmented variants\n",
    "                    # We pass the p_... values to the augmentation function\n",
    "                    variants = create_enhanced_augmented_variants_progressive(\n",
    "                        original_ast, \n",
    "                        num_augs, \n",
    "                        strategies,\n",
    "                        plan_config\n",
    "                    )\n",
    "                    \n",
    "                    base_name = json_file.replace(\".json\", \"\")\n",
    "                    for i, variant in enumerate(variants):\n",
    "                        if i == 0:\n",
    "                            output_name = f\"{base_name}_original.json\"\n",
    "                        else:\n",
    "                            output_name = f\"{base_name}_aug{i}.json\"\n",
    "                        \n",
    "                        output_path = os.path.join(output_cipher_dir, output_name)\n",
    "                        save_json(variant, output_path)\n",
    "                        total_graphs += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing {json_file}: {e}\")\n",
    "                    \n",
    "        print(f\"✅ Finished {plan_name}. Total graphs: {total_graphs}\")\n",
    "\n",
    "\n",
    "def create_enhanced_augmented_variants_progressive(original_ast: Dict[str, Any], \n",
    "                                                   num_variants: int, \n",
    "                                                   strategies: List,\n",
    "                                                   plan_config: Dict) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Modified creation function that accepts progressive scaling parameters.\n",
    "    \"\"\"\n",
    "    variants = [copy.deepcopy(original_ast)] # Include original\n",
    "    \n",
    "    # Get parameters from the plan\n",
    "    p_node_drop = plan_config['node_dropout_p']\n",
    "    p_edge_drop = plan_config['edge_perturb_p']\n",
    "    p_edge_add = plan_config['edge_perturb_p'] / 2 # Add fewer than we drop\n",
    "    noise_lvl = plan_config['noise_level']\n",
    "\n",
    "    for i in range(num_variants):\n",
    "        augmented_ast = copy.deepcopy(original_ast)\n",
    "        \n",
    "        num_augmentations = random.randint(2, 4) # Apply 2-4 strategies per variant\n",
    "        applied_strategies_fns = weighted_strategy_sampling(strategies, num_augmentations)\n",
    "        \n",
    "        applied_strategies_names = []\n",
    "\n",
    "        for strategy_fn in applied_strategies_fns:\n",
    "            try:\n",
    "                # This is how you pass parameters to specific functions\n",
    "                if strategy_fn == safe_node_dropout_cryptographic:\n",
    "                    augmented_ast = strategy_fn(augmented_ast, p=p_node_drop)\n",
    "                elif strategy_fn == safe_edge_perturbation_cryptographic:\n",
    "                    augmented_ast = strategy_fn(augmented_ast, p_drop=p_edge_drop, p_add=p_edge_add)\n",
    "                elif strategy_fn == safe_feature_noise_injection:\n",
    "                    augmented_ast = strategy_fn(augmented_ast, noise_level=noise_lvl)\n",
    "                else:\n",
    "                    augmented_ast = strategy_fn(augmented_ast)\n",
    "                \n",
    "                applied_strategies_names.append(strategy_fn.__name__)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Strategy {strategy_fn.__name__} failed: {e}\")\n",
    "        \n",
    "        # Recompute stats AFTER all augmentations\n",
    "        recompute_op_counts_from_nodes_and_edges(augmented_ast)\n",
    "        recompute_graph_stats(augmented_ast)\n",
    "        \n",
    "        # Update metadata\n",
    "        if \"pdv\" not in augmented_ast: augmented_ast[\"pdv\"] = {}\n",
    "        augmented_ast[\"pdv\"][\"augmented\"] = True\n",
    "        augmented_ast[\"pdv\"][\"augmentation_metadata\"] = {\n",
    "            \"strategies\": applied_strategies_names,\n",
    "            \"variant_id\": i + 1\n",
    "        }\n",
    "        \n",
    "        # Preserve labels\n",
    "        if \"security_score\" in original_ast:\n",
    "            augmented_ast[\"security_score\"] = original_ast[\"security_score\"]\n",
    "        if \"security_label\" in original_ast:\n",
    "            augmented_ast[\"security_label\"] = original_ast[\"security_label\"]\n",
    "        \n",
    "        variants.append(augmented_ast)\n",
    "    \n",
    "    return variants\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Run the progressive augmentation to create your new datasets\n",
    "    run_progressive_augmentation()\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c553db-4ad5-442b-be83-21fa772fb3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758abc11-35ff-4128-9fb7-57851cf2cddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e82e2-f470-447a-a6ba-607c49edb058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3848db-3d41-4c2b-81b2-748c873d6072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e39b5ede-978c-45c6-a987-27fa0eb8ec45",
   "metadata": {},
   "source": [
    "### SAMPLING THE DATASET:\n",
    "\n",
    "Organized output to sampled_data_variant_based_balanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0985f517-d159-4a74-8ebf-9435962551a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Data Sampling for Dataset Balance\n",
    "# ----------------------------\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "FLAG = 0\n",
    "\n",
    "class CipherDataSampler:\n",
    "    def __init__(self, input_base_dir: str, output_base_dir: str, delta_percent: float = 0.2):\n",
    "        random.seed(42)\n",
    "        np.random.seed(42)\n",
    "        self.input_base_dir = input_base_dir\n",
    "        self.output_base_dir = output_base_dir\n",
    "        self.delta_percent = delta_percent\n",
    "        self.label_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "        self.reverse_label_mapping = {v: k for k, v in self.label_mapping.items()}\n",
    "\n",
    "    # ----------------------------\n",
    "    # Loading & Label Standardizing\n",
    "    # ----------------------------\n",
    "    def load_all_json_files(self) -> List[Dict[str, Any]]:\n",
    "        all_files = []\n",
    "        cipher_dirs = [\n",
    "            d for d in os.listdir(self.input_base_dir)\n",
    "            if os.path.isdir(os.path.join(self.input_base_dir, d))\n",
    "        ]\n",
    "\n",
    "        for cipher_dir in cipher_dirs:\n",
    "            cipher_path = os.path.join(self.input_base_dir, cipher_dir)\n",
    "            json_files = [f for f in os.listdir(cipher_path) if f.endswith(\".json\")]\n",
    "            for json_file in json_files:\n",
    "                file_path = os.path.join(cipher_path, json_file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)\n",
    "                        data[\"file_source\"] = file_path\n",
    "                        data[\"cipher\"] = cipher_dir\n",
    "                        # make sure cipher_variant exists (fallback to file name without .json)\n",
    "                        if \"cipher_variant\" not in data:\n",
    "                            base = os.path.splitext(json_file)[0]\n",
    "                            data[\"cipher_variant\"] = base\n",
    "                        all_files.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"X -- Error loading {file_path}: {e}\")\n",
    "        print(f\" OK -- Loaded {len(all_files)} total files from {len(cipher_dirs)} ciphers.\")\n",
    "        return all_files\n",
    "\n",
    "    def standardize_labels(self, data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        for data in data_list:\n",
    "            label = data.get(\"security_label\", \"low\")\n",
    "            if isinstance(label, str):\n",
    "                data[\"standardized_label\"] = self.label_mapping.get(label.lower(), 0)\n",
    "            else:\n",
    "                data[\"standardized_label\"] = int(label)\n",
    "        return data_list\n",
    "\n",
    "    # ----------------------------\n",
    "    # Build structure: cipher -> variant -> list(files)\n",
    "    # Each file should indicate whether it's augmented\n",
    "    # ----------------------------\n",
    "    def build_cipher_variant_index(self, data_list: List[Dict[str, Any]]) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:\n",
    "        idx = defaultdict(lambda: defaultdict(list))\n",
    "        for d in data_list:\n",
    "            cipher = d.get(\"cipher\", \"unknown\")\n",
    "            variant = d.get(\"cipher_variant\", \"unknown_variant\")\n",
    "            idx[cipher][variant].append(d)\n",
    "        return idx\n",
    "\n",
    "    # ----------------------------\n",
    "    # Utility: gather variant->label mapping (assume variant's label = label of original file)\n",
    "    # If multiple originals exist (rare), choose the non-augmented one as canonical.\n",
    "    # ----------------------------\n",
    "    def variant_label(self, files: List[Dict[str, Any]]) -> int:\n",
    "        # prefer a non-augmented file's label as the variant's label\n",
    "        for f in files:\n",
    "            if not f.get(\"pdv\", {}).get(\"augmented\", False):\n",
    "                return f.get(\"standardized_label\", 0)\n",
    "        # otherwise return first file label\n",
    "        return files[0].get(\"standardized_label\", 0)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Core algorithm: per-cipher sampling guided by samples_per_variant (applied to smallest-class variants)\n",
    "    # ----------------------------\n",
    "    def sample_cipher_for_samples_per_variant(self, variants: Dict[str, List[Dict[str, Any]]],\n",
    "                                              samples_per_variant: int, FLAG=FLAG) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        variants: dict variant_name -> list[file dicts]\n",
    "        Goal:\n",
    "          - Determine variant counts per class\n",
    "          - baseline = min_variant_count * samples_per_variant\n",
    "          - baseline_with_delta = baseline + randint(0, int(baseline * delta_percent))\n",
    "          - For each class:\n",
    "              target_files = max(number_of_original_files_in_class, baseline_with_delta)\n",
    "              select: all originals first, then add augmented files by variant up to samples_per_variant per variant\n",
    "        Returns list of selected file dicts for the cipher.\n",
    "        \"\"\"\n",
    "        # Organize variants by class label\n",
    "        class_variants = defaultdict(list)  # label -> list of variant names\n",
    "        variant_files = variants  # variant_name -> list of files\n",
    "\n",
    "        # Build per-variant canonical label and per-variant split(originals, augmented)\n",
    "        variant_meta = {}\n",
    "        for vname, files in variant_files.items():\n",
    "            label = self.variant_label(files)\n",
    "            # separate originals vs augmented for this variant\n",
    "            originals = [f for f in files if not f.get(\"pdv\", {}).get(\"augmented\", False)]\n",
    "            augmented = [f for f in files if f.get(\"pdv\", {}).get(\"augmented\", False)]\n",
    "            variant_meta[vname] = {\"label\": label, \"originals\": originals, \"augmented\": augmented}\n",
    "            class_variants[label].append(vname)\n",
    "\n",
    "        # If some class has zero variants, skip it\n",
    "        classes_present = list(class_variants.keys())\n",
    "        if not classes_present:\n",
    "            return []\n",
    "            \n",
    "        FLAG += 1\n",
    "        # min number of variants across present classes (we consider only classes that appear)\n",
    "        min_variant_count = min(len(class_variants[c]) for c in classes_present)\n",
    "\n",
    "        # baseline files = min_variant_count * samples_per_variant\n",
    "        baseline = min_variant_count * samples_per_variant\n",
    "        # add-only delta\n",
    "        add_delta = random.randint(0, max(1, int(baseline * self.delta_percent)))\n",
    "        add_delta2 = max(FLAG%2, add_delta)\n",
    "        \n",
    "        baseline_with_delta = baseline + add_delta\n",
    "\n",
    "        #print('baseline_with_delta, baseline_with_delta, baseline + add_delta2, add_delta2, add_delta1')\n",
    "        #print('baseline_with_delta', baseline_with_delta, baseline + add_delta2, add_delta2, add_delta)\n",
    "        selected_files = []\n",
    "\n",
    "        # For each class, compute target files and select\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        # For each class, compute target files and select\n",
    "        for label in classes_present:\n",
    "            variant_names = class_variants[label]\n",
    "            # number of originals available in class (one per variant typically)\n",
    "            originals_list = []\n",
    "            for v in variant_names:\n",
    "                originals_list.extend(variant_meta[v][\"originals\"])\n",
    "\n",
    "            num_original_files = len(originals_list)  # typically equals number of variants for this class\n",
    "\n",
    "            # target_files for this class: at least baseline_with_delta, but never below num_original_files\n",
    "            target_files = max(num_original_files, baseline_with_delta)\n",
    "\n",
    "            # But cap target_files by total available files in this class\n",
    "            total_available = num_original_files + sum(len(variant_meta[v][\"augmented\"]) for v in variant_names)\n",
    "            target_files = min(target_files, total_available)\n",
    "\n",
    "            # Step 1: include all originals (preserve originals)\n",
    "            selected_class_files = list(originals_list)\n",
    "\n",
    "            # Step 2: if we need more, add augmented files while respecting per-variant per-limit\n",
    "            needed = target_files - len(selected_class_files)\n",
    "            if needed > 0:\n",
    "                # For fairness, iterate variants in round-robin order adding up to per-variant limit:\n",
    "                # Each variant may contribute up to (samples_per_variant - selected_from_that_variant)\n",
    "                per_variant_selected = {v: 0 for v in variant_names}\n",
    "                # initialize per-variant selected count from originals (if original from that variant included)\n",
    "                for v in variant_names:\n",
    "                    # if variant had an original and it's included, count it\n",
    "                    if variant_meta[v][\"originals\"]:\n",
    "                        per_variant_selected[v] = min(1, samples_per_variant)  # original counted as one\n",
    "                # gather pools of augmented per variant\n",
    "                aug_pools = {v: list(variant_meta[v][\"augmented\"]) for v in variant_names}\n",
    "\n",
    "                # round-robin across variants to avoid concentrating all added files in a single variant\n",
    "                variant_cycle = variant_names.copy()\n",
    "                vi = 0\n",
    "                iteration_count = 0\n",
    "                max_iterations = len(variant_cycle) * 100  # Safety limit\n",
    "                \n",
    "                # print(f\"    [DEBUG] Starting round-robin selection, max_iterations={max_iterations}\")\n",
    "                \n",
    "                # FIX: Create a list of variants that can still contribute\n",
    "                active_variants = [v for v in variant_cycle \n",
    "                                  if per_variant_selected[v] < samples_per_variant and aug_pools[v]]\n",
    "\n",
    " \n",
    "                while needed > 0 and active_variants and iteration_count < max_iterations:\n",
    "                    v = active_variants[vi % len(active_variants)]\n",
    "                    vi += 1\n",
    "                    iteration_count += 1\n",
    "                    \n",
    "                    # This condition should always be true because of active_variants filtering\n",
    "                    if per_variant_selected[v] < samples_per_variant and aug_pools[v]:\n",
    "                        selected_class_files.append(aug_pools[v].pop(0))\n",
    "                        per_variant_selected[v] += 1\n",
    "                        needed -= 1\n",
    "                        # print(f\"    [DEBUG] Iteration {iteration_count}: Added from variant {v}, needed={needed}\")\n",
    "                    \n",
    "                    # Update active_variants - remove variants that can no longer contribute\n",
    "                    active_variants = [v for v in active_variants \n",
    "                                      if per_variant_selected[v] < samples_per_variant and aug_pools[v]]\n",
    "                    \n",
    "                    # Safety check - if we're stuck, break\n",
    "                    if iteration_count >= max_iterations:\n",
    "                        print(f\"    [WARNING] Reached max iterations ({max_iterations}), breaking loop\")\n",
    "                        break\n",
    "                \n",
    "                # print(f\"    [DEBUG] After round-robin: needed={needed}, active_variants={len(active_variants)}\")\n",
    "                \n",
    "                # if still needed (rare), try adding remaining augmented from any variant ignoring per-variant cap\n",
    "                if needed > 0:\n",
    "                    leftover = []\n",
    "                    for v in variant_names:\n",
    "                        leftover.extend(aug_pools[v])\n",
    "                    if leftover:\n",
    "                        to_take = min(needed, len(leftover))\n",
    "                        selected_class_files.extend(leftover[:to_take])\n",
    "                        needed -= to_take\n",
    "\n",
    "            # done for this class: add to global selection\n",
    "            selected_files.extend(selected_class_files)\n",
    "\n",
    "        # end for each class\n",
    "        return selected_files\n",
    "\n",
    "    # ----------------------------\n",
    "    # Main multi-size generation (per-variant sampling but balancing via baseline)\n",
    "    # ----------------------------\n",
    "    def generate_datasets_for_variant_sizes(self, samples_per_variant_sizes: List[int], FLAG=FLAG):\n",
    "        all_data = self.standardize_labels(self.load_all_json_files())\n",
    "        # index: cipher -> variant -> list(files)\n",
    "        cipher_variant_index = defaultdict(dict)\n",
    "        raw_index = self.build_cipher_variant_index(all_data)\n",
    "        for cipher, variants in raw_index.items():\n",
    "            for vname, files in variants.items():\n",
    "                cipher_variant_index[cipher][vname] = files\n",
    "\n",
    "        for size in samples_per_variant_sizes:\n",
    "            print(f\"\\n ### Generating dataset for samples_per_variant = {size} ###\")\n",
    "            output_dir = os.path.join(self.output_base_dir, f\"samples_per_variant_{size}\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            # print(\"FOLDER CREATED\")\n",
    "\n",
    "            for cipher, variants in cipher_variant_index.items():\n",
    "                FLAG +=1\n",
    "                cipher_dir = os.path.join(output_dir, cipher)\n",
    "                os.makedirs(cipher_dir, exist_ok=True)\n",
    "                # print( \"folder created :\", cipher_dir ) \n",
    "\n",
    "                sampled_files = self.sample_cipher_for_samples_per_variant(variants, size )\n",
    "                # print( \"fALL SAMPLE FILES CREATED:\" ) \n",
    "                \n",
    "\n",
    "                # Save all samples for this cipher (no subfolders per class)\n",
    "                for i, sample in enumerate(sampled_files):\n",
    "                    if \"file_source\" in sample:\n",
    "                        # print( \"fALL SAMPLE FILES CREATED:\" ) \n",
    "                        del sample[\"file_source\"]\n",
    "                    label_name = self.reverse_label_mapping[sample[\"standardized_label\"]]\n",
    "                    is_aug = \"_aug\" if sample.get(\"pdv\", {}).get(\"augmented\", False) else \"\"\n",
    "                    variant = sample.get(\"cipher_variant\", f\"{cipher}_unknown\")\n",
    "                    filename = f\"{variant}_{label_name}{is_aug}_{i:03d}.json\"\n",
    "                    filepath = os.path.join(cipher_dir, filename)\n",
    "                    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(sample, f, indent=2)\n",
    "\n",
    "                print(f\"  - {cipher}: saved {len(sampled_files)} files → {cipher_dir}\")\n",
    "\n",
    "            print(f\" OK -- Finished dataset for {size} samples per variant → {output_dir}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Usage \n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    sampler = CipherDataSampler(\n",
    "        input_base_dir=\"augmented_data_V6\",\n",
    "        output_base_dir=\"sampled_data_variant_based_balanced_V6\",\n",
    "        delta_percent=0.4\n",
    "    )\n",
    "\n",
    "    # Different sizes you want per variant (applied to smallest-class variants)\n",
    "    samples_per_variant_sizes = [1, 2,3,4, 5, 6, 7, 8,9, 10 ]\n",
    "    sampler.generate_datasets_for_variant_sizes(samples_per_variant_sizes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0f451-bbf1-4b14-b8e0-14a0a96e3fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ec5c9-68ba-462a-aabb-52c6139be335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
