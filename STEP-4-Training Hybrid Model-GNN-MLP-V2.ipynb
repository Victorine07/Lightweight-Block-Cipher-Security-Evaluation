{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd4b3fb-a416-4d2b-b147-0b1c7078e990",
   "metadata": {},
   "source": [
    "### STEP-4-TRAINING HYBRID MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0295d-78ea-43aa-b1c3-85756f24dcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf81cf-3958-49b3-90b5-67754d064ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723ff43-8420-4b0f-9f63-6ecc24febedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "# 1. DEPRECATION FIX: Import DataLoader from 'loader'\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, TransformerConv, global_mean_pool\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# =============================================================================\n",
    "# 1. COMPREHENSIVE PDV FEATURE EXTRACTION (Unchanged)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_enhanced_pdv_features(json_data: Dict[str, Any]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Extracts a comprehensive, flat feature vector from unified_pdv.\n",
    "    This version includes ALL numerical metrics from unified_pdv for the MLP.\n",
    "    \"\"\"\n",
    "    unified_pdv = json_data.get(\"unified_pdv\", {})\n",
    "    \n",
    "    # This fixed-order list ensures every vector is consistent\n",
    "    pdv_feature_keys = [\n",
    "        \"block_size\", \"key_size\", \"rounds\", \"is_feistel\", \"is_arx\", \"is_spn\",\n",
    "        \"xor_count\", \"rotl_count\", \"rotr_count\", \"add_count\", \"sub_count\", \n",
    "        \"and_count\", \"sbox_count\", \"perm_count\", \"round_complexity\", \n",
    "        \"rotation_diversity\", \"max_rotation_amount\", \"has_round_function\", \n",
    "        \"has_f_function\", \"has_enc_round\", \"has_dec_round\", \"has_key_schedule\", \n",
    "        \"ast_node_count\", \"ast_edge_count\", \"function_count\", \"uses_z_sequence\", \n",
    "        \"uses_shift_params\", \"uses_sbox\", \"uses_permutation\", \n",
    "        \"round_function_size\", \"operations_per_round\", \"complexity_ratio\", \n",
    "        \"estimated_total_operations\", \"key_schedule_operations\", \n",
    "        \"encryption_operations\", \"decryption_operations\", \"f_function_richness\", \n",
    "        \"nonlinearity_density\", \"diffusion_strength\", \"operation_diversity\", \n",
    "        \"crypto_strength_score\", \"cryptographic_pattern_score\", \n",
    "        \"data_flow_complexity\", \"feistel_balance\", \"key_schedule_complexity\", \n",
    "        \"arx_balance_score\", \"sbox_strength\", \"confusion_diffusion_ratio\", \n",
    "        \"layer_separation\", \"bidirectional_diffusion\"\n",
    "    ]\n",
    "    \n",
    "    all_features = []\n",
    "    for key in pdv_feature_keys:\n",
    "        value = unified_pdv.get(key, 0.0)\n",
    "        \n",
    "        # Handle potential string values like \"simple\" or \"complex\"\n",
    "        if not isinstance(value, (int, float)):\n",
    "            if key == \"key_schedule_complexity\":\n",
    "                if value == \"simple\": value = 1.0\n",
    "                elif value == \"complex\": value = 3.0\n",
    "                else: value = 0.0\n",
    "            else:\n",
    "                value = 0.0  # Default for other unexpected strings\n",
    "                \n",
    "        all_features.append(float(value))\n",
    "\n",
    "    # Also add the raw security score as a feature\n",
    "    all_features.append(float(json_data.get(\"security_score\", 0.0)))\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "# =============================================================================\n",
    "# 2. ROBUST GRAPH DATA ENCODER (Unchanged)\n",
    "# =============================================================================\n",
    "class GraphDataEncoder:\n",
    "    \"\"\"\n",
    "    V2: This version now correctly parses the 'cryptographic_patterns'\n",
    "    dictionary from 'function' nodes and adds them to the feature vector.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Categorical feature mappings\n",
    "        self.type_map = {}\n",
    "        self.role_map = {}\n",
    "        self.flow_role_map = {}\n",
    "        self.context_map = {}\n",
    "        self.edge_type_map = {}\n",
    "        \n",
    "        # Numerical feature counts\n",
    "        self.base_numerical_features = 3  # strength, diffusion, nonlinearity\n",
    "        \n",
    "        # ## NEW: Define the number of features we will extract from function patterns ##\n",
    "        # This will be 10 features, see _encode_node for details\n",
    "        self.function_pattern_features = 10 \n",
    "        \n",
    "        # Final dimensions\n",
    "        self.node_feat_dim = 0\n",
    "        self.pdv_feat_dim = 0\n",
    "        self.edge_attr_dim = 1\n",
    "\n",
    "    def _build_mapping(self, items: List[str], unknown_key=\"unknown\") -> Dict[str, int]:\n",
    "        unique_items = sorted(list(set(items)))\n",
    "        mapping = {item: i for i, item in enumerate(unique_items)}\n",
    "        if unknown_key not in mapping:\n",
    "            mapping[unknown_key] = len(mapping)\n",
    "        return mapping\n",
    "\n",
    "    def _one_hot(self, key: str, mapping: Dict[str, int], unknown_key=\"unknown\") -> List[float]:\n",
    "        vec = [0.0] * len(mapping)\n",
    "        idx = mapping.get(key, mapping.get(unknown_key))\n",
    "        if idx is not None:\n",
    "            vec[idx] = 1.0\n",
    "        return vec\n",
    "\n",
    "    def fit(self, all_json_data: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Fits the encoder by scanning all JSON files to build\n",
    "        complete categorical feature mappings.\n",
    "        \"\"\"\n",
    "        print(\"Fitting encoder on dataset...\")\n",
    "        all_types, all_roles, all_flow_roles, all_contexts, all_edge_types = [], [], [], [], []\n",
    "\n",
    "        for js in all_json_data:\n",
    "            for node in js.get(\"nodes\", []):\n",
    "                all_types.append(node.get(\"type\", \"unknown\"))\n",
    "                all_roles.append(node.get(\"crypto_role\", \"unknown\"))\n",
    "                all_flow_roles.append(node.get(\"data_flow_role\", \"unknown\"))\n",
    "                all_contexts.append(node.get(\"context\", \"unknown\"))\n",
    "            for edge in js.get(\"edges\", []):\n",
    "                all_edge_types.append(edge.get(\"type\", \"unknown\"))\n",
    "\n",
    "        # Build mappings\n",
    "        self.type_map = self._build_mapping(all_types)\n",
    "        self.role_map = self._build_mapping(all_roles)\n",
    "        self.flow_role_map = self._build_mapping(all_flow_roles)\n",
    "        self.context_map = self._build_mapping(all_contexts)\n",
    "        self.edge_type_map = self._build_mapping(all_edge_types)\n",
    "\n",
    "        # Calculate categorical dimension\n",
    "        categorical_dim = (\n",
    "            len(self.type_map) +\n",
    "            len(self.role_map) +\n",
    "            len(self.flow_role_map) +\n",
    "            len(self.context_map)\n",
    "        )\n",
    "        \n",
    "        # Calculate TOTAL node dimension\n",
    "        self.node_feat_dim = (\n",
    "            self.base_numerical_features +\n",
    "            self.function_pattern_features +  # <-- NEW\n",
    "            categorical_dim\n",
    "        )\n",
    "        \n",
    "        # Get PDV dim\n",
    "        if all_json_data:\n",
    "            self.pdv_feat_dim = len(extract_enhanced_pdv_features(all_json_data[0]))\n",
    "        \n",
    "        print(f\"‚úÖ Encoder fit complete.\")\n",
    "        print(f\"  > Base Numerical Dims:     {self.base_numerical_features}\")\n",
    "        print(f\"  > Function Pattern Dims:   {self.function_pattern_features}\")\n",
    "        print(f\"  > Categorical Dims:      {categorical_dim} (from {len(self.type_map)} types, {len(self.role_map)} roles, etc.)\")\n",
    "        print(f\"  > TOTAL NODE FEAT DIM: {self.node_feat_dim}\")\n",
    "        print(f\"  > PDV feat dim:            {self.pdv_feat_dim}\")\n",
    "        print(f\"  > Edge types:              {len(self.edge_type_map)}\")\n",
    "\n",
    "\n",
    "    def _encode_node(self, node: Dict[str, Any]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Encodes a single node using the fitted mappings.\n",
    "        V2: Now includes function pattern features.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Base Numerical Features (3 Dims)\n",
    "        numerical_vec = [\n",
    "            float(node.get(\"crypto_strength\", 0.0)),\n",
    "            float(node.get(\"diffusion_power\", 0.0)),\n",
    "            float(node.get(\"nonlinearity\", 0.0))\n",
    "        ]\n",
    "        \n",
    "        # 2. NEW: Function-Specific Numerical Features (10 Dims)\n",
    "        function_vec = [0.0] * self.function_pattern_features\n",
    "        if node.get(\"type\") == \"function\":\n",
    "            patterns = node.get(\"cryptographic_patterns\", {})\n",
    "            op_dist = patterns.get(\"cryptographic_operation_distribution\", {})\n",
    "            \n",
    "            function_vec = [\n",
    "                1.0 if patterns.get(\"feistel_network\") else 0.0,\n",
    "                len(patterns.get(\"arx_operation_chains\", [])),\n",
    "                1.0 if patterns.get(\"spn_layers\") else 0.0,\n",
    "                float(patterns.get(\"key_schedule_complexity\", 0.0)),\n",
    "                float(patterns.get(\"feistel_rounds_detected\", 0.0)),\n",
    "                len(patterns.get(\"round_structures\", [])),\n",
    "                float(op_dist.get(\"linear_mixing\", 0.0)),\n",
    "                float(op_dist.get(\"nonlinear_mixing\", 0.0)),\n",
    "                float(op_dist.get(\"diffusion\", 0.0)),\n",
    "                float(op_dist.get(\"operation\", 0.0))\n",
    "            ]\n",
    "        \n",
    "        # 3. Categorical (One-Hot) Features (e.g., 101 Dims)\n",
    "        type_vec = self._one_hot(node.get(\"type\", \"unknown\"), self.type_map)\n",
    "        role_vec = self._one_hot(node.get(\"crypto_role\", \"unknown\"), self.role_map)\n",
    "        flow_vec = self._one_hot(node.get(\"data_flow_role\", \"unknown\"), self.flow_role_map)\n",
    "        context_vec = self._one_hot(node.get(\"context\", \"unknown\"), self.context_map)\n",
    "        \n",
    "        # Concatenate all features into a single, fixed-size vector\n",
    "        features = (\n",
    "            numerical_vec +\n",
    "            function_vec +\n",
    "            type_vec + \n",
    "            role_vec + \n",
    "            flow_vec + \n",
    "            context_vec\n",
    "        )\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def transform(self, js: Dict[str, Any]) -> Data:\n",
    "        \"\"\"\n",
    "        Transforms a single JSON object into a PyG Data object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- PDV Features ---\n",
    "        pdv_features = extract_enhanced_pdv_features(js)\n",
    "        if self.pdv_feat_dim > 0 and len(pdv_features) != self.pdv_feat_dim:\n",
    "            raise ValueError(f\"PDV feature dimension mismatch for {js.get('cipher_variant')}\")\n",
    "        pdv = torch.tensor([pdv_features], dtype=torch.float)\n",
    "\n",
    "        # --- Node Features ---\n",
    "        nodes_data = js.get(\"nodes\", [])\n",
    "        if not nodes_data:\n",
    "            dummy_node = {\"type\": \"unknown\", \"crypto_role\": \"unknown\", \"data_flow_role\": \"unknown\", \"context\": \"unknown\"}\n",
    "            node_feats = [self._encode_node(dummy_node)]\n",
    "            nodes_data = [dummy_node]\n",
    "        else:\n",
    "            node_feats = [self._encode_node(node) for node in nodes_data]\n",
    "            \n",
    "        x = torch.tensor(node_feats, dtype=torch.float)\n",
    "        \n",
    "        # --- Sanity Check ---\n",
    "        if x.shape[1] != self.node_feat_dim:\n",
    "            # This should not happen if .fit() was called, but it's a good safeguard\n",
    "            raise RuntimeError(f\"Feature dimension mismatch! Encoder.dim={self.node_feat_dim} but created vector of size={x.shape[1]}\")\n",
    "\n",
    "        # --- Edge Index (CRITICAL FIX) ---\n",
    "        id_to_index = {node['id']: i for i, node in enumerate(nodes_data) if 'id' in node}\n",
    "        edges = js.get(\"edges\", [])\n",
    "        edge_sources, edge_targets, edge_attrs = [], [], []\n",
    "        \n",
    "        for e in edges:\n",
    "            source_id, target_id = e.get(\"source\"), e.get(\"target\")\n",
    "            edge_type = e.get(\"type\", \"unknown\")\n",
    "            \n",
    "            if source_id in id_to_index and target_id in id_to_index:\n",
    "                edge_sources.append(id_to_index[source_id])\n",
    "                edge_targets.append(id_to_index[target_id])\n",
    "                edge_type_idx = self.edge_type_map.get(edge_type, self.edge_type_map.get(\"unknown\", 0))\n",
    "                edge_attrs.append([float(edge_type_idx)])\n",
    "        \n",
    "        if edge_sources:\n",
    "            edge_index = torch.tensor([edge_sources, edge_targets], dtype=torch.long)\n",
    "            edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "        else:\n",
    "            edge_index = torch.tensor([[], []], dtype=torch.long)\n",
    "            edge_attr = torch.empty((0, self.edge_attr_dim), dtype=torch.float)\n",
    "\n",
    "        # --- Label ---\n",
    "        label_map = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "        security_label = js.get(\"security_label\", \"low\")\n",
    "        y = torch.tensor([label_map.get(security_label, 0)], dtype=torch.long)\n",
    "\n",
    "        # --- Create Data Object ---\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, pdv=pdv)\n",
    "        data.cipher = js.get(\"cipher_variant\", \"unknown\")\n",
    "        data.filename = js.get(\"source_file\", \"unknown\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "# =============================================================================\n",
    "# 3. UPDATED DATA LOADING PIPELINE (Unchanged)\n",
    "# =============================================================================\n",
    "\n",
    "def load_all_json_data(sampled_data_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"First pass: Load all JSON data from subdirectories into memory.\"\"\"\n",
    "    all_json_data = []\n",
    "    print(f\"Scanning for JSON files in {sampled_data_dir}...\")\n",
    "    for cipher_dir in os.listdir(sampled_data_dir):\n",
    "        cipher_path = os.path.join(sampled_data_dir, cipher_dir)\n",
    "        if not os.path.isdir(cipher_path):\n",
    "            continue\n",
    "            \n",
    "        json_files = [os.path.join(cipher_path, f) for f in os.listdir(cipher_path) \n",
    "                      if f.endswith(\".json\") and not f.startswith('_')]\n",
    "        \n",
    "        for f in json_files:\n",
    "            try:\n",
    "                with open(f, \"r\") as jf:\n",
    "                    js = json.load(jf)\n",
    "                if isinstance(js, list) or \"nodes\" not in js or \"unified_pdv\" not in js:\n",
    "                    print(f\"‚ö†Ô∏è Skipping {f}: Incomplete or old format.\")\n",
    "                    continue\n",
    "                js['source_file_path'] = f\n",
    "                all_json_data.append(js)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error reading {f}: {e}\")\n",
    "                \n",
    "    print(f\"Found {len(all_json_data)} valid JSON files.\")\n",
    "    return all_json_data\n",
    "\n",
    "\n",
    "def load_enhanced_graphs_from_dataset(sampled_data_dir: str):\n",
    "    \"\"\"\n",
    "    Loads all graphs and returns the processed Data objects and the fitted encoder.\n",
    "    \"\"\"\n",
    "    all_json_data = load_all_json_data(sampled_data_dir)\n",
    "    if not all_json_data:\n",
    "        raise ValueError(\"No valid JSON data found in directory.\")\n",
    "\n",
    "    encoder = GraphDataEncoder()\n",
    "    encoder.fit(all_json_data)\n",
    "    \n",
    "    graphs = []\n",
    "    for js in all_json_data:\n",
    "        try:\n",
    "            data = encoder.transform(js)\n",
    "            graphs.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error transforming {js.get('source_file_path')}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(graphs)} graphs with enhanced features.\")\n",
    "    \n",
    "    # !!! ADD THIS LINE from your last request !!!\n",
    "    print(f\"üìä Label Distribution (0=low, 1=medium, 2=high): {np.bincount([d.y.item() for d in graphs])}\")\n",
    "    \n",
    "    return graphs, encoder\n",
    "\n",
    "\n",
    "## MODIFIED ##\n",
    "def prepare_test_cipher_enhanced(json_file_path: str, encoder: GraphDataEncoder) -> Dict:\n",
    "    \"\"\"\n",
    "    Loads and transforms a single, unseen cipher (like HIGHT)\n",
    "    using the *already-fitted* encoder.\n",
    "    \"\"\"\n",
    "    print(f\"\\nPreparing unseen test cipher: {json_file_path}\")\n",
    "    try:\n",
    "        with open(json_file_path, \"r\") as jf:\n",
    "            js = json.load(jf)\n",
    "\n",
    "        # ## MODIFIED: Check if the test file has a label ##\n",
    "        if \"security_label\" not in js:\n",
    "            print(f\"‚ö†Ô∏è Test file {json_file_path} has no 'security_label'. Cannot check correctness.\")\n",
    "            true_label_str = \"unknown\"\n",
    "        else:\n",
    "            true_label_str = js.get(\"security_label\")\n",
    "            \n",
    "        data = encoder.transform(js)\n",
    "        test_loader = DataLoader([data], batch_size=1)\n",
    "        batched_data = next(iter(test_loader))\n",
    "        \n",
    "        label_map = {\"low\": 0, \"medium\": 1, \"high\": 2, \"unknown\": -1}\n",
    "        label_names = {v: k for k, v in label_map.items()}\n",
    "        \n",
    "        return {\n",
    "            'gnn_data': batched_data,\n",
    "            'true_label': label_map.get(true_label_str), \n",
    "            'label_names': label_names,\n",
    "            'cipher_name': os.path.basename(json_file_path).replace('.json', '')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error preparing test cipher {json_file_path}: {e}\")\n",
    "        return None\n",
    "        \n",
    "# =============================================================================\n",
    "# 4. HYBRID MODEL ARCHITECTURES (Dynamic Layers - Unchanged)\n",
    "# =============================================================================\n",
    "\n",
    "def _build_mlp(in_dim: int, hidden_dims: List[int], dropout: float) -> nn.Sequential:\n",
    "    \"\"\"Helper function to build a dynamic MLP.\"\"\"\n",
    "    layers = nn.ModuleList()\n",
    "    for dim in hidden_dims:\n",
    "        layers.append(nn.Linear(in_dim, dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        in_dim = dim\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class FocusedGCNModel(nn.Module):\n",
    "    def __init__(self, node_feat_dim, pdv_dim, n_classes, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Build Dynamic GNN\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_dim = node_feat_dim\n",
    "        for out_dim in config['gcn_dims']:\n",
    "            self.convs.append(GCNConv(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "        self.gnn_out_dim = config['gcn_dims'][-1]\n",
    "        \n",
    "        # Build Dynamic PDV MLP\n",
    "        self.pdv_mlp = _build_mlp(pdv_dim, config['pdv_dims'], config['dropout'])\n",
    "        self.pdv_out_dim = config['pdv_dims'][-1] if config['pdv_dims'] else pdv_dim # Handle empty pdv_dims\n",
    "\n",
    "        # Build Dynamic Classifier\n",
    "        combined_in_dim = self.gnn_out_dim + self.pdv_out_dim\n",
    "        self.classifier = _build_mlp(combined_in_dim, config['classifier_dims'], config['dropout'])\n",
    "        \n",
    "        # Add final output layer\n",
    "        classifier_in_dim = config['classifier_dims'][-1] if config['classifier_dims'] else combined_in_dim\n",
    "        self.classifier.add_module(\"output_layer\", nn.Linear(classifier_in_dim, n_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.config['dropout'], training=self.training)\n",
    "        \n",
    "        ast_out = global_mean_pool(x, batch)\n",
    "        pdv_out = self.pdv_mlp(data.pdv.view(-1, data.pdv.shape[-1]))\n",
    "            \n",
    "        combined = torch.cat([ast_out, pdv_out], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "\n",
    "class FocusedGATModel(nn.Module):\n",
    "    def __init__(self, node_feat_dim, pdv_dim, n_classes, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        heads = config['heads']\n",
    "\n",
    "        # Build Dynamic GNN\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_dim = node_feat_dim\n",
    "        for i, out_dim in enumerate(config['gat_dims']):\n",
    "            is_last_layer = (i == len(config['gat_dims']) - 1)\n",
    "            layer_heads = 1 if is_last_layer else heads\n",
    "            concat = not is_last_layer\n",
    "            self.convs.append(GATConv(in_dim, out_dim, heads=layer_heads, concat=concat))\n",
    "            in_dim = out_dim * layer_heads\n",
    "        self.gnn_out_dim = config['gat_dims'][-1]\n",
    "\n",
    "        # Build Dynamic PDV MLP\n",
    "        self.pdv_mlp = _build_mlp(pdv_dim, config['pdv_dims'], config['dropout'])\n",
    "        self.pdv_out_dim = config['pdv_dims'][-1] if config['pdv_dims'] else pdv_dim\n",
    "\n",
    "        # Build Dynamic Classifier\n",
    "        combined_in_dim = self.gnn_out_dim + self.pdv_out_dim\n",
    "        self.classifier = _build_mlp(combined_in_dim, config['classifier_dims'], config['dropout'])\n",
    "        \n",
    "        classifier_in_dim = config['classifier_dims'][-1] if config['classifier_dims'] else combined_in_dim\n",
    "        self.classifier.add_module(\"output_layer\", nn.Linear(classifier_in_dim, n_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = F.elu(x)\n",
    "                x = F.dropout(x, p=self.config['dropout'], training=self.training)\n",
    "        \n",
    "        ast_out = global_mean_pool(x, batch)\n",
    "        pdv_out = self.pdv_mlp(data.pdv.view(-1, data.pdv.shape[-1]))\n",
    "            \n",
    "        combined = torch.cat([ast_out, pdv_out], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "\n",
    "class FocusedTransformerModel(nn.Module):\n",
    "    def __init__(self, node_feat_dim, pdv_dim, edge_attr_dim, n_classes, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        heads = config['heads']\n",
    "\n",
    "        # Build Dynamic GNN\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_dim = node_feat_dim\n",
    "        for i, out_dim in enumerate(config['transformer_dims']):\n",
    "            is_last_layer = (i == len(config['transformer_dims']) - 1)\n",
    "            layer_heads = 1 if is_last_layer else heads\n",
    "            concat = not is_last_layer\n",
    "            self.convs.append(TransformerConv(in_dim, out_dim, heads=layer_heads, \n",
    "                                              concat=concat, edge_dim=edge_attr_dim))\n",
    "            in_dim = out_dim * layer_heads\n",
    "        self.gnn_out_dim = config['transformer_dims'][-1]\n",
    "\n",
    "        # Build Dynamic PDV MLP\n",
    "        self.pdv_mlp = _build_mlp(pdv_dim, config['pdv_dims'], config['dropout'])\n",
    "        self.pdv_out_dim = config['pdv_dims'][-1] if config['pdv_dims'] else pdv_dim\n",
    "\n",
    "        # Build Dynamic Classifier\n",
    "        combined_in_dim = self.gnn_out_dim + self.pdv_out_dim\n",
    "        self.classifier = _build_mlp(combined_in_dim, config['classifier_dims'], config['dropout'])\n",
    "        \n",
    "        classifier_in_dim = config['classifier_dims'][-1] if config['classifier_dims'] else combined_in_dim\n",
    "        self.classifier.add_module(\"output_layer\", nn.Linear(classifier_in_dim, n_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index, data.edge_attr)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = F.elu(x)\n",
    "                x = F.dropout(x, p=self.config['dropout'], training=self.training)\n",
    "        \n",
    "        ast_out = global_mean_pool(x, batch)\n",
    "        pdv_out = self.pdv_mlp(data.pdv.view(-1, data.pdv.shape[-1]))\n",
    "            \n",
    "        combined = torch.cat([ast_out, pdv_out], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. FOCUSED GRID TRAINING MANAGER (MODIFIED)\n",
    "# =============================================================================\n",
    "\n",
    "class FocusedGridTrainingManager:\n",
    "    ## MODIFIED: Changed constructor to take a directory\n",
    "    def __init__(self, base_data_dir=\"sampled_data_variant_based_balanced_V6\", \n",
    "                 test_ciphers_dir=\"test_only_ciphers\"):\n",
    "        self.base_data_dir = base_data_dir\n",
    "        self.test_ciphers_dir = test_ciphers_dir # <-- MODIFIED\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.results = {}\n",
    "        self.encoder = None \n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def get_model(self, model_type, node_feat_dim, pdv_dim, edge_attr_dim, n_classes, config):\n",
    "        if model_type == 'GCN':\n",
    "            return FocusedGCNModel(node_feat_dim, pdv_dim, n_classes, config)\n",
    "        elif model_type == 'GAT':\n",
    "            return FocusedGATModel(node_feat_dim, pdv_dim, n_classes, config)\n",
    "        elif model_type == 'Transformer':\n",
    "            return FocusedTransformerModel(node_feat_dim, pdv_dim, edge_attr_dim, n_classes, config)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    ## MODIFIED: Added 3-fold split for n_samples < 50\n",
    "    def get_training_config(self, n_samples, model_config):\n",
    "        \"\"\"Get training configuration optimized for dataset size\"\"\"\n",
    "        \n",
    "        if n_samples < 50:  # dataset-3 (45 samples)\n",
    "            batch_size = max(4, n_samples // 8)\n",
    "            n_splits = 3  # Use 3 folds for a more stable validation set (15 samples)\n",
    "        elif n_samples < 80:  # dataset sizes 4-5\n",
    "            batch_size = max(8, n_samples // 8)\n",
    "            n_splits = 5\n",
    "        else:  # dataset sizes 6-8\n",
    "            batch_size = max(12, n_samples // 10)\n",
    "            n_splits = 5\n",
    "        \n",
    "        return {\n",
    "            'batch_size': batch_size,\n",
    "            'n_epochs': model_config['n_epochs'], \n",
    "            'patience': model_config['patience'],\n",
    "            'lr': model_config['lr'],\n",
    "            'weight_decay': model_config['weight_decay'],\n",
    "            'n_splits': n_splits\n",
    "        }\n",
    "\n",
    "    def train_single_fold(self, model, train_loader, val_loader, config):\n",
    "        model = model.to(self.device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=config['lr'], \n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # 2. DEPRECATION FIX: Removed 'verbose=False'\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, patience=config['patience']//3, factor=0.5\n",
    "        )\n",
    "        \n",
    "        best_val_f1 = -1.0\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(config['n_epochs']):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch in train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch)\n",
    "                loss = criterion(out, batch.y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            val_f1, _ = self.evaluate_model(model, val_loader)\n",
    "            scheduler.step(epoch_loss / len(train_loader))\n",
    "            \n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= config['patience']:\n",
    "                break\n",
    "        \n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        \n",
    "        return best_val_f1, model\n",
    "\n",
    "    def evaluate_model(self, model, loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = batch.to(self.device)\n",
    "                out = model(batch)\n",
    "                pred = out.argmax(dim=1)\n",
    "                y_true.extend(batch.y.cpu().tolist())\n",
    "                y_pred.extend(pred.cpu().tolist())\n",
    "        \n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0.0)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        return f1, acc\n",
    "\n",
    "    ## MODIFIED: Renamed function and added loop ##\n",
    "    def test_on_unseen_ciphers(self, model, encoder):\n",
    "        \"\"\"Test trained model on all ciphers in the test directory\"\"\"\n",
    "        \n",
    "        test_results = {}\n",
    "        \n",
    "        if not os.path.exists(self.test_ciphers_dir):\n",
    "            print(f\"‚ùå Test directory not found: {self.test_ciphers_dir}\")\n",
    "            return None\n",
    "\n",
    "        test_files = [f for f in os.listdir(self.test_ciphers_dir) if f.endswith('.json')]\n",
    "        if not test_files:\n",
    "            print(f\"‚ö†Ô∏è No .json files found in {self.test_ciphers_dir}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"  üéØ TESTING ON {len(test_files)} UNSEEN CIPHERS:\")\n",
    "        \n",
    "        for test_file in test_files:\n",
    "            file_path = os.path.join(self.test_ciphers_dir, test_file)\n",
    "            test_data_package = prepare_test_cipher_enhanced(file_path, encoder)\n",
    "            \n",
    "            if test_data_package is None:\n",
    "                continue\n",
    "                \n",
    "            test_data = test_data_package['gnn_data'].to(self.device)\n",
    "            true_label = test_data_package['true_label']\n",
    "            label_names = test_data_package['label_names']\n",
    "            cipher_name = test_data_package['cipher_name']\n",
    "\n",
    "            try:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    output = model(test_data)\n",
    "                    probabilities = F.softmax(output, dim=1)\n",
    "                    prediction = output.argmax(dim=1).cpu().item()\n",
    "                    confidence = probabilities.max().cpu().item()\n",
    "                    confidence_per_class = probabilities.cpu().numpy()[0]\n",
    "                \n",
    "                correct = (prediction == true_label) \n",
    "                \n",
    "                result_key = f\"test_{cipher_name}\"\n",
    "                test_results[result_key] = {\n",
    "                    'prediction': prediction,\n",
    "                    'prediction_label': label_names[prediction],\n",
    "                    'confidence': confidence,\n",
    "                    'confidence_low': confidence_per_class[0],\n",
    "                    'confidence_medium': confidence_per_class[1],\n",
    "                    'confidence_high': confidence_per_class[2],\n",
    "                    'correct': bool(correct)\n",
    "                }\n",
    "                \n",
    "                status = \"‚úÖ CORRECT\" if correct else \"‚ùå WRONG\"\n",
    "                if true_label == -1: status = \"?? UNKNOWN\"\n",
    "                print(f\"     {cipher_name:20}: Pred={label_names[prediction]:<7} - {status}\")\n",
    "                print(test_results[result_key])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {cipher_name} test failed: {e}\")\n",
    "                print(test_results[result_key])\n",
    "                \n",
    "                \n",
    "        return test_results\n",
    "\n",
    "    ## MODIFIED: Updated to handle the new test results dictionary ##\n",
    "    def run_focused_grid_training(self, dataset_sizes=None, model_types=None):\n",
    "        if dataset_sizes is None:\n",
    "            dataset_sizes = [8, 15, 30]\n",
    "        if model_types is None:\n",
    "            model_types = ['GCN', 'GAT', 'Transformer']\n",
    "        \n",
    "        print(\"üéØ FOCUSED GRID TRAINING (Sweet Spot: Datasets 3-5)\")\n",
    "        print(f\"üìä Dataset Sizes: {dataset_sizes}\")\n",
    "        print(f\"ü§ñ Model Types: {model_types}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        total_configs = len(dataset_sizes) * len(model_types) * 5\n",
    "        current_config = 0\n",
    "        \n",
    "        for dataset_size in dataset_sizes:\n",
    "            dataset_dir = f\"{self.base_data_dir}/samples_per_variant_{dataset_size}\"\n",
    "            \n",
    "            if not os.path.exists(dataset_dir):\n",
    "                print(f\"‚ùå Dataset directory not found: {dataset_dir}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                graphs, self.encoder = load_enhanced_graphs_from_dataset(dataset_dir)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load dataset {dataset_dir}: {e}\")\n",
    "                continue\n",
    "                \n",
    "            if len(graphs) < 10:\n",
    "                print(f\"‚ö†Ô∏è Skipping size {dataset_size}: only {len(graphs)} graphs\")\n",
    "                continue\n",
    "                \n",
    "            node_feat_dim = self.encoder.node_feat_dim\n",
    "            pdv_dim = self.encoder.pdv_feat_dim\n",
    "            edge_attr_dim = self.encoder.edge_attr_dim\n",
    "            n_classes = 3\n",
    "            \n",
    "            print(f\"\\nüì¶ Dataset Size {dataset_size}: {len(graphs)} graphs\")\n",
    "            print(f\"üìê Node Dim: {node_feat_dim}, PDV Dim: {pdv_dim}, Edge Dim: {edge_attr_dim}\")\n",
    "            \n",
    "            for model_type in model_types:\n",
    "                configs = FOCUSED_GRID_CONFIGS[model_type]\n",
    "                \n",
    "                for config_idx, config in enumerate(configs):\n",
    "                    current_config += 1\n",
    "                    config_id = f\"size{dataset_size}_{model_type}_config{config_idx+1}\"\n",
    "                    \n",
    "                    print(f\"\\nüîß [{current_config}/{total_configs}] Training {config_id}\")\n",
    "                    \n",
    "                    training_config = self.get_training_config(len(graphs), config)\n",
    "                    \n",
    "                    kf = KFold(n_splits=training_config['n_splits'], shuffle=True, random_state=42)\n",
    "                    cv_f1_scores, cv_acc_scores, fold_models = [], [], []\n",
    "                    \n",
    "                    print(f\"  üìÅ CROSS-VALIDATION ({training_config['n_splits']} folds):\")\n",
    "                    \n",
    "                    for fold, (train_idx, val_idx) in enumerate(kf.split(graphs)):\n",
    "                        if len(val_idx) < 2: continue\n",
    "                        \n",
    "                        train_loader = DataLoader([graphs[i] for i in train_idx], \n",
    "                                                  batch_size=training_config['batch_size'], shuffle=True)\n",
    "                        val_loader = DataLoader([graphs[i] for i in val_idx], \n",
    "                                                batch_size=min(training_config['batch_size'], len(val_idx)))\n",
    "                        \n",
    "                        model = self.get_model(model_type, node_feat_dim, pdv_dim, edge_attr_dim, n_classes, config)\n",
    "                        \n",
    "                        _, trained_model = self.train_single_fold(\n",
    "                            model, train_loader, val_loader, training_config\n",
    "                        )\n",
    "                        \n",
    "                        final_f1, final_acc = self.evaluate_model(trained_model, val_loader)\n",
    "                        \n",
    "                        cv_f1_scores.append(final_f1)\n",
    "                        cv_acc_scores.append(final_acc)\n",
    "                        fold_models.append(trained_model)\n",
    "                        \n",
    "                        print(f\"    Fold {fold+1}: Val F1 = {final_f1:.4f}, Val Acc = {final_acc:.4f}\")\n",
    "                    \n",
    "                    if cv_f1_scores:\n",
    "                        mean_f1, std_f1 = np.mean(cv_f1_scores), np.std(cv_f1_scores)\n",
    "                        mean_acc, std_acc = np.mean(cv_acc_scores), np.std(cv_acc_scores)\n",
    "                        \n",
    "                        print(f\"  üìà CROSS-VALIDATION RESULTS:\")\n",
    "                        print(f\"     Mean F1: {mean_f1:.4f} ¬± {std_f1:.4f}\")\n",
    "                        print(f\"     Mean Acc: {mean_acc:.4f} ¬± {std_acc:.4f}\")\n",
    "                        \n",
    "                        best_fold_idx = np.argmax(cv_f1_scores)\n",
    "                        best_model = fold_models[best_fold_idx]\n",
    "                        \n",
    "                        os.makedirs(f\"{OUTPUT_MODELS_SAVED}\", exist_ok=True)\n",
    "                        model_path = f\"{OUTPUT_MODELS_SAVED}/{config_id}_best.pt\"\n",
    "                        torch.save({\n",
    "                            'state_dict': best_model.state_dict(), 'config': config,\n",
    "                            'mean_f1': mean_f1, 'mean_acc': mean_acc,\n",
    "                            'dataset_size': dataset_size, 'model_type': model_type,\n",
    "                            'node_feat_dim': node_feat_dim, 'pdv_dim': pdv_dim,\n",
    "                            'edge_attr_dim': edge_attr_dim\n",
    "                        }, model_path)\n",
    "                        \n",
    "                        # ## MODIFIED: Call new test function ##\n",
    "                        test_results_dict = self.test_on_unseen_ciphers(best_model, self.encoder)\n",
    "                        \n",
    "                        # Store all results\n",
    "                        self.results[config_id] = {\n",
    "                            'dataset_size': dataset_size, 'model_type': model_type,\n",
    "                            'config_idx': config_idx, 'mean_f1': mean_f1, 'std_f1': std_f1,\n",
    "                            'mean_acc': mean_acc, 'std_acc': std_acc,\n",
    "                            'architecture': config, 'n_samples': len(graphs),\n",
    "                            'node_feat_dim': node_feat_dim, 'pdv_dim': pdv_dim,\n",
    "                            'unseen_test_results': test_results_dict # <-- MODIFIED\n",
    "                        }\n",
    "                    else:\n",
    "                        print(f\"  ‚ùå {config_id}: No valid folds completed\")\n",
    "            \n",
    "        print(f\"\\nüéâ FOCUSED GRID TRAINING COMPLETED!\")\n",
    "        print(f\"üìä Trained {len(self.results)} configurations\")\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "# =============================================================================\n",
    "# 6. RESULTS ANALYSIS (HEAVILY MODIFIED)\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_focused_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and display the best configurations.\n",
    "    ## MODIFIED: This function now flattens the 'unseen_test_results'\n",
    "    dictionary into separate columns for analysis.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ùå No results to analyze\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nüìä FOCUSED RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results_list = []\n",
    "    all_test_cipher_names = set()\n",
    "\n",
    "    # First, flatten the results and find all unique test ciphers\n",
    "    for config_id, result in results.items():\n",
    "        row = result.copy()\n",
    "        row['config_id'] = config_id\n",
    "        \n",
    "        # Pop the nested test results\n",
    "        test_results = row.pop('unseen_test_results', {})\n",
    "        \n",
    "        if test_results:\n",
    "            correct_count = 0\n",
    "            for test_name, test_data in test_results.items():\n",
    "                all_test_cipher_names.add(test_name)\n",
    "                # Add flattened data to the row\n",
    "                row[f\"{test_name}_pred\"] = test_data['prediction_label']\n",
    "                row[f\"{test_name}_correct\"] = test_data['correct']\n",
    "                row[f\"{test_name}_confidence\"] = test_data['confidence']\n",
    "                if test_data['correct']:\n",
    "                    correct_count += 1\n",
    "            row['generalization_score'] = f\"{correct_count}/{len(test_results)}\"\n",
    "            row['generalization_correct'] = correct_count\n",
    "            row['generalization_total'] = len(test_results)\n",
    "        \n",
    "        results_list.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"‚ùå Results DataFrame is empty. No configurations ran successfully.\")\n",
    "        return None\n",
    "    \n",
    "    # Define columns to display\n",
    "    # 'dataset_size' is now included in base_cols\n",
    "    base_cols = ['config_id', 'dataset_size', 'mean_f1', 'std_f1', 'mean_acc', 'std_acc', 'generalization_correct', 'generalization_total','model_type']\n",
    "    \n",
    "    # --- END OF FIX ---\n",
    "    \n",
    "    #base_cols = ['config_id', 'mean_f1', 'std_f1', 'mean_acc', 'std_acc']\n",
    "    test_cols = []\n",
    "    for name in sorted(list(all_test_cipher_names)):\n",
    "        test_cols.extend([f\"{name}_pred\", f\"{name}_correct\", f\"{name}_confidence\"])\n",
    "\n",
    "    # Ensure all columns exist, fill with NaN if they don't\n",
    "    for col in base_cols + test_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "            \n",
    "    # Reorder columns for clarity\n",
    "    df = df[base_cols + ['generalization_score'] + test_cols + ['architecture']]\n",
    "\n",
    "    print(\"\\nüèÜ BEST OVERALL CONFIGURATIONS:\")\n",
    "    \n",
    "    # Best F1 score\n",
    "    best_f1 = df.loc[df['mean_f1'].idxmax()]\n",
    "    print(f\"üéØ Best F1 Score: {best_f1['config_id']}\")\n",
    "    print(f\"  F1: {best_f1['mean_f1']:.4f} ¬± {best_f1['std_f1']:.4f} (Acc: {best_f1['mean_acc']:.4f})\")\n",
    "    print(f\"  Generalization: {best_f1['generalization_score']}\")\n",
    "    \n",
    "    # Best Generalization\n",
    "    if 'generalization_correct' in df.columns:\n",
    "        best_gen_df = df[df['generalization_correct'] == df['generalization_correct'].max()]\n",
    "        best_gen = best_gen_df.loc[best_gen_df['mean_f1'].idxmax()]\n",
    "        print(f\"üéØ Best Generalization Performance (with highest F1): {best_gen['config_id']}\")\n",
    "        print(f\"  Generalization: {best_gen['generalization_score']}\")\n",
    "        print(f\"  F1: {best_gen['mean_f1']:.4f} ¬± {best_gen['std_f1']:.4f} (Acc: {best_gen['mean_acc']:.4f})\")\n",
    "    else:\n",
    "        print(\"üéØ No generalization results to report.\")\n",
    "    \n",
    "    print(f\"\\nüìà PERFORMANCE BY DATASET SIZE:\")\n",
    "    for size in sorted(df['dataset_size'].unique()):\n",
    "        size_results = df[df['dataset_size'] == size]\n",
    "        avg_f1 = size_results['mean_f1'].mean()\n",
    "        avg_gen_correct = size_results['generalization_correct'].mean()\n",
    "        total_gen = size_results['generalization_total'].iloc[0] if not size_results.empty else 0\n",
    "        print(f\"  Size {size}: Avg F1={avg_f1:.4f}, Avg Generalization={avg_gen_correct:.2f}/{total_gen}\")\n",
    "    \n",
    "    print(f\"\\nü§ñ PERFORMANCE BY MODEL TYPE:\")\n",
    "    for model_type in df['model_type'].unique():\n",
    "        model_results = df[df['model_type'] == model_type]\n",
    "        avg_f1 = model_results['mean_f1'].mean()\n",
    "        avg_gen_correct = model_results['generalization_correct'].mean()\n",
    "        total_gen = model_results['generalization_total'].iloc[0] if not model_results.empty else 0\n",
    "        best_config = model_results.loc[model_results['mean_f1'].idxmax()]['config_id']\n",
    "        print(f\"  {model_type:12}: Avg F1={avg_f1:.4f}, Avg Generalization={avg_gen_correct:.2f}/{total_gen}\")\n",
    "        print(f\"     Best config: {best_config}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# 7. MAIN EXECUTION PIPELINE (MODIFIED)\n",
    "# =============================================================================\n",
    "\n",
    "def _clean_results_for_json(data):\n",
    "    \"\"\"Recursively clean a dictionary of numpy types for JSON serialization.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {k: _clean_results_for_json(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [_clean_results_for_json(v) for v in data]\n",
    "    elif isinstance(data, np.floating):\n",
    "        return float(data)\n",
    "    elif isinstance(data, np.integer):\n",
    "        return int(data)\n",
    "    elif isinstance(data, np.bool_):\n",
    "        return bool(data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def run_focused_grid_pipeline():\n",
    "    \"\"\"Run the complete focused grid training pipeline\"\"\"\n",
    "    print(\"üöÄ FOCUSED GRID TRAINING PIPELINE (V3 - Multi-Test)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ## MODIFIED: Pass the test directory name ##\n",
    "    trainer = FocusedGridTrainingManager(\n",
    "        base_data_dir=\"sampled_data_variant_based_balanced_V6\",\n",
    "        test_ciphers_dir=\"test_only_ciphers/HIGHT\"\n",
    "    )\n",
    "    \n",
    "    results = trainer.run_focused_grid_training(\n",
    "        dataset_sizes= [1,2,3,4, 5,6,7, 8,9, 10],\n",
    "        \n",
    "        model_types=['GCN', 'GAT', 'Transformer']\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        df = analyze_focused_results(results)\n",
    "        \n",
    "        if df is not None:\n",
    "            df.to_csv(\"focused_grid_results.csv\", index=False)\n",
    "            print(f\"\\nüíæ Results saved to: focused_grid_results.csv\")\n",
    "            \n",
    "            # ## MODIFIED: Use the recursive cleaning function for safe JSON dump ##\n",
    "            with open(\"focused_grid_detailed_results.json\", \"w\") as f:\n",
    "                cleaned_results = _clean_results_for_json(results)\n",
    "                json.dump(cleaned_results, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Detailed results saved to: focused_grid_detailed_results.json\")\n",
    "            print(f\"üíæ Models saved to: best_models_focused/ directory\")\n",
    "            \n",
    "            return df, results\n",
    "        else:\n",
    "            print(\"‚ùå Analysis failed, no CSV saved.\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"‚ùå Training failed - no results generated\")\n",
    "        return None, None\n",
    "\n",
    "# =============================================================================\n",
    "# FOCUSED GRID CONFIGURATIONS (V6 - \"Wide & Shallow\" Edition)\n",
    "#\n",
    "# NEW INSIGHT: Node feature dimension is 104. We must fix the input bottleneck.\n",
    "# HYPOTHESIS: The best model is a 2-LAYER GNN (to prevent oversmoothing)\n",
    "#             with a FIRST LAYER > 104 (to prevent bottlenecking).\n",
    "#             We will tune width and regularization.\n",
    "# =============================================================================\n",
    "\n",
    "OUTPUT_MODELS_SAVED = 'best_models_focused_V6'\n",
    "FOCUSED_GRID_CONFIGS = {\n",
    "    'GCN': [\n",
    "        # Config 1: Baseline (Wide 2-Layer)\n",
    "        {'gcn_dims': [128, 72], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "         'lr': 0.0008, 'dropout': 0.4, 'weight_decay': 1e-4, 'patience': 20, 'n_epochs': 150},\n",
    "        \n",
    "        # Config 2: Wider GNN\n",
    "        {'gcn_dims': [192, 96], 'pdv_dims': [64, 48], 'classifier_dims': [128, 64],\n",
    "        'lr': 0.0005, 'dropout': 0.5, 'weight_decay': 5e-4, 'patience': 20, 'n_epochs': 150},\n",
    "\n",
    "        # Config 3: Widest GNN\n",
    "        {'gcn_dims': [256, 128], 'pdv_dims': [64, 48], 'classifier_dims': [128, 64],\n",
    "        'lr': 0.0005, 'dropout': 0.5, 'weight_decay': 5e-4, 'patience': 20, 'n_epochs': 150},\n",
    "\n",
    "        # Config 4: Baseline with less dropout (to boost F1)\n",
    "        {'gcn_dims': [128, 92, 88], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "         'lr': 0.001, 'dropout': 0.35, 'weight_decay': 3e-4, 'patience': 20, 'n_epochs': 150},\n",
    "        \n",
    "        # Config 4: Baseline with less dropout (to boost F1)\n",
    "        {'gcn_dims': [128, 92, 80], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "         'lr': 0.0009, 'dropout': 0.35, 'weight_decay': 2.5e-4, 'patience': 20, 'n_epochs': 150},\n",
    "\n",
    "        # Config 5: Baseline with more dropout (to boost generalization)\n",
    "        {'gcn_dims': [128, 64], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "        'lr': 0.0005, 'dropout': 0.6, 'weight_decay': 5e-4, 'patience': 20, 'n_epochs': 150},\n",
    "    ],\n",
    "    \n",
    "    'GAT': [\n",
    "        # Config 1: Baseline GAT (Wide 2-Layer, 2 Heads)\n",
    "        {'gat_dims': [128, 64], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "         'heads': 2, 'lr': 0.0005, 'dropout': 0.6, 'weight_decay': 5e-4, 'patience': 25, 'n_epochs': 150},\n",
    "\n",
    "        # Config 2: Baseline GAT (Wide 2-Layer, 4 Heads)\n",
    "        {'gat_dims': [128, 88], 'pdv_dims': [64, 48], 'classifier_dims': [88, 48],\n",
    "         'heads': 2, 'lr': 0.0005, 'dropout': 0.3, 'weight_decay': 4e-4, 'patience': 25, 'n_epochs': 150},\n",
    "\n",
    "        # Config 3: Wider GAT (4 Heads)\n",
    "        {'gat_dims': [128, 92], 'pdv_dims': [64, 32], 'classifier_dims': [112, 72, 48],\n",
    "         'heads': 4, 'lr': 0.0005, 'dropout': 0.35, 'weight_decay': 3e-4, 'patience': 25, 'n_epochs': 150},\n",
    "\n",
    "        \n",
    "        # Config 4: Baseline GAT (4 Heads) with less dropout\n",
    "        {'gat_dims': [128, 88], 'pdv_dims': [64, 48], 'classifier_dims': [128, 96, 48],\n",
    "         'heads': 4, 'lr': 0.0005, 'dropout': 0.3, 'weight_decay': 2.5e-4, 'patience': 25, 'n_epochs': 150},\n",
    "        \n",
    "        # Config 4: Baseline GAT (4 Heads) with less dropout\n",
    "        {'gat_dims': [120, 92], 'pdv_dims': [64, 32], 'classifier_dims': [96, 48],\n",
    "         'heads': 4, 'lr': 0.0005, 'dropout': 0.35, 'weight_decay': 3e-4, 'patience': 25, 'n_epochs': 150},\n",
    "\n",
    "        \n",
    "        # Config 5: Baseline GAT (4 Heads) with more heads\n",
    "        {'gat_dims': [128, 64], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "         'heads': 6, 'lr': 0.0005, 'dropout': 0.6, 'weight_decay': 5e-4, 'patience': 25, 'n_epochs': 150},\n",
    "    ],\n",
    "    \n",
    "    'Transformer': [\n",
    "        # Config 1: Baseline Transformer (Wide 2-Layer, 2 Heads)\n",
    "        {'transformer_dims': [156, 96, 72], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "         'heads': 4, 'lr': 0.0005, 'dropout': 0.4, 'weight_decay': 3e-4, 'patience': 25, 'n_epochs': 150},\n",
    "\n",
    "        \n",
    "        {'transformer_dims': [128, 104, 80], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "         'heads': 4, 'lr': 0.0005, 'dropout': 0.35, 'weight_decay': 2e-4, 'patience': 25, 'n_epochs': 150},\n",
    "\n",
    "        # Config 2: Baseline Transformer (Wide 2-Layer, 4 Heads)\n",
    "        {'transformer_dims': [128, 64], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "         'heads': 4, 'lr': 0.0005, 'dropout': 0.6, 'weight_decay': 5e-4, 'patience': 25, 'n_epochs': 150},\n",
    "        \n",
    "        # Config 3: Wider Transformer (4 Heads)\n",
    "        {'transformer_dims': [192, 96], 'pdv_dims': [64, 48], 'classifier_dims': [128, 64],\n",
    "         'heads': 4, 'lr': 0.0005, 'dropout': 0.6, 'weight_decay': 5e-4, 'patience': 25, 'n_epochs': 150},\n",
    "\n",
    "        # Config 4: Baseline Transformer (4 Heads) with less dropout\n",
    "        {'transformer_dims': [128, 72 ], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "         'heads': 4, 'lr': 0.0008, 'dropout': 0.35, 'weight_decay': 3e-4, 'patience': 25, 'n_epochs': 150},\n",
    "\n",
    "        \n",
    "        {'transformer_dims': [128, 104, 80], 'pdv_dims': [64, 48], 'classifier_dims': [96, 64, 48],\n",
    "         'heads': 4, 'lr': 0.0005, 'dropout': 0.35, 'weight_decay': 3e-4, 'patience': 25, 'n_epochs': 150},\n",
    "        \n",
    "        # Config 5: Baseline Transformer (4 Heads) with more heads\n",
    "        {'transformer_dims': [128, 64], 'pdv_dims': [64, 48], 'classifier_dims': [96, 48],\n",
    "         'heads': 6, 'lr': 0.0005, 'dropout': 0.6, 'weight_decay': 5e-4, 'patience': 25, 'n_epochs': 150},\n",
    "    ]\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use the V3 (Data-Scarce) configs to avoid overfitting\n",
    "\n",
    "    results_df, detailed_results = run_focused_grid_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89b92a-89ed-4582-a319-3b05cc5d5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df)\n",
    "print(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af673f83-533c-4ba4-9112-878ff6fa8e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a65e18-3350-40ce-8a4c-3bd1565e3ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406ff2e-1154-46a4-8d08-89f18e2c62f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
